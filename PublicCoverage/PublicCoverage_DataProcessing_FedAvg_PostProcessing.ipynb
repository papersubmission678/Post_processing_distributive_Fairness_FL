{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T18:24:04.808469Z",
     "start_time": "2024-09-04T18:24:04.805573Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.src.metrics import accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbedb99c26675407",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T18:24:04.831157Z",
     "start_time": "2024-09-04T18:24:04.827183Z"
    }
   },
   "outputs": [],
   "source": [
    "from folktables import ACSDataSource, ACSPublicCoverage\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "data_source = ACSDataSource(survey_year='2018', horizon='1-Year', survey='person')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7409ae407a777f46",
   "metadata": {},
   "source": [
    "# Load Data for All US States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3bb97f57612c07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T18:24:39.365213Z",
     "start_time": "2024-09-04T18:24:04.864792Z"
    }
   },
   "outputs": [],
   "source": [
    "# List of all 50 US state abbreviations\n",
    "states = [\n",
    "    \"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DE\", \"FL\", \"GA\",\n",
    "    \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\",\n",
    "    \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\",\n",
    "    \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\",\n",
    "    \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\", \"WY\"\n",
    "]\n",
    "# states = [\"CA\", \"TX\"]  # Example subset of states\n",
    "\n",
    "# Dictionary to store data, features, and labels for each state\n",
    "state_data = {}\n",
    "state_features = {}\n",
    "state_labels = {}\n",
    "state_groups = {}\n",
    "\n",
    "# Assuming `data_source` and `ACSIncome` are already defined\n",
    "for state in states:\n",
    "    # Load data for each state\n",
    "    state_data[state] = data_source.get_data(states=[state], download=True)\n",
    "    # Convert to features and labels\n",
    "    state_features[state], state_labels[state], state_groups[state]= ACSPublicCoverage.df_to_numpy(state_data[state])\n",
    "    state_features[state] = scaler.fit_transform(state_features[state])\n",
    "    state_groups[state] = np.where(state_groups[state] == 1, 0, 1)\n",
    "    print(state, state_features[state].shape, state_labels[state].shape,state_groups[state].shape)\n",
    "\n",
    "# Example usage: Access data, features, and labels for California (CA)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ecfcc057d93c0",
   "metadata": {},
   "source": [
    "# Split Data into Train, Validation, and Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105236c0fa9e4b79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T18:24:39.548044Z",
     "start_time": "2024-09-04T18:24:39.396463Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Define dictionaries to store train, validation, and test data for each state\n",
    "train_features = {}\n",
    "train_labels = {}\n",
    "train_groups = {}\n",
    "val_features = {}\n",
    "val_labels = {}\n",
    "vali_groups = {}\n",
    "test_features = {}\n",
    "test_labels = {}\n",
    "test_groups = {}\n",
    "all_test_features = []\n",
    "all_test_labels = []\n",
    "all_test_groups = []\n",
    "all_vaild_features = []\n",
    "all_valid_labels = []\n",
    "all_valid_groups = []\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Split ratio configuration\n",
    "train_ratio = 0.5\n",
    "val_ratio = 0.3\n",
    "test_ratio = 0.2\n",
    "\n",
    "for state in states:\n",
    "    # Get the features and labels for the state\n",
    "    features = state_features[state]\n",
    "    labels = state_labels[state]\n",
    "    groups = state_groups[state]\n",
    "\n",
    "    # First split into train and temp (validation + test) sets\n",
    "    X_train, X_temp, y_train, y_temp, s_train, s_temp = train_test_split(\n",
    "        features, labels, groups, test_size=(1 - train_ratio), random_state=42)\n",
    "\n",
    "    # Then split the temp set into validation and test sets\n",
    "    X_val, X_test, y_val, y_test, s_val, s_test = train_test_split(\n",
    "        X_temp, y_temp, s_temp, test_size=(test_ratio / (val_ratio + test_ratio)), random_state=42)\n",
    "\n",
    "    # Store the splits into respective dictionaries\n",
    "    train_features[state] = X_train\n",
    "    train_labels[state] = y_train\n",
    "    train_groups[state] = s_train\n",
    "    val_features[state] = X_val\n",
    "    val_labels[state] = y_val\n",
    "    vali_groups[state] = s_val\n",
    "    test_features[state] = X_test\n",
    "    test_labels[state] = y_test\n",
    "    test_groups[state] = s_test\n",
    "    all_vaild_features.append(X_val)\n",
    "    all_valid_labels.append(y_val)\n",
    "    all_valid_groups.append(s_val)\n",
    "    all_test_features.append(X_test)\n",
    "    all_test_labels.append(y_test)\n",
    "    all_test_groups.append(s_test)\n",
    "\n",
    "    # Print the shapes for verification\n",
    "    print(f\"{state}: Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "all_test_features = np.concatenate(all_test_features)\n",
    "all_test_labels = np.concatenate(all_test_labels)\n",
    "all_test_groups = np.concatenate(all_test_groups)\n",
    "all_vaild_features=np.concatenate(all_vaild_features)\n",
    "all_vaild_labels=np.concatenate(all_valid_labels)\n",
    "all_valid_groups=np.concatenate(all_valid_groups)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7eed158e9e4202c",
   "metadata": {},
   "source": [
    "# Train a FedAvg model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14dfe09d29fd8e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T18:30:50.769540Z",
     "start_time": "2024-09-04T18:24:39.580013Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# List of all 50 US state abbreviations\n",
    "\n",
    "# Define a simple neural network model\n",
    "def create_model(input_shape):\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.InputLayer(input_shape=input_shape),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Training parameters\n",
    "global_epochs = 10\n",
    "local_epochs = 1\n",
    "batch_size = 256\n",
    "\n",
    "# Initialize global model\n",
    "input_shape = state_features[states[0]].shape[1:]  # Assuming all states have the same feature shape\n",
    "global_model = create_model(input_shape)\n",
    "\n",
    "# Federated Averaging\n",
    "for epoch in range(global_epochs):\n",
    "    start_time=time.time()\n",
    "    print(f\"Global Epoch {epoch + 1}/{global_epochs}\")\n",
    "    \n",
    "    # Store the weights from each local model\n",
    "    local_weights = []\n",
    "\n",
    "    # Train model on each state's data\n",
    "    for state in states:\n",
    "        # print(f\"Training on state: {state}\")\n",
    "\n",
    "        # Create a local model for each state\n",
    "        local_model = create_model(input_shape)\n",
    "        local_model.set_weights(global_model.get_weights())  # Initialize with global model weights\n",
    "\n",
    "        # Train local model\n",
    "        local_model.fit(\n",
    "            state_features[state], state_labels[state],\n",
    "            epochs=local_epochs, batch_size=batch_size\n",
    "        )\n",
    "\n",
    "        # Append the local weights\n",
    "        local_weights.append(local_model.get_weights())\n",
    "\n",
    "    # Average the weights to get new global weights\n",
    "    new_global_weights = [np.mean([local_weights[j][i] for j in range(len(local_weights))], axis=0)\n",
    "                          for i in range(len(local_weights[0]))]\n",
    "    end_time=time.time()\n",
    "    # print('time:',end_time-start_time)\n",
    "    # Set the new global weights\n",
    "    global_model.set_weights(new_global_weights)\n",
    "    loss,accuracy=global_model.evaluate(all_vaild_features,all_vaild_labels)\n",
    "    print('validation accuray:',accuracy)\n",
    "# Evaluate the global model\n",
    "print(\"\\nEvaluating Global Model\")\n",
    "for state in states:\n",
    "    loss, accuracy = global_model.evaluate(val_features[state], val_labels[state], verbose=0)\n",
    "    print(f\"{state} - Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6052d99501ad0b73",
   "metadata": {},
   "source": [
    "# local and global accuracy, fairness (EOp) of FedAvg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae937d3341d0e99a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T18:31:05.305226Z",
     "start_time": "2024-09-04T18:30:50.806420Z"
    }
   },
   "outputs": [],
   "source": [
    "y_val_pred={}\n",
    "local_EO=[]\n",
    "local_acc=[]\n",
    "def compute_equal_odd(y_true, y_pred, groups):\n",
    "    white_tpr_1 = np.mean(y_pred[(y_true == 1) & (groups == 0)])\n",
    "    black_tpr_1 = np.mean(y_pred[(y_true == 1) & (groups == 1)])\n",
    "    white_tpr_0 = np.mean((1-y_pred)[(y_true == 0) & (groups == 0)])\n",
    "    black_tpr_0 = np.mean((1-y_pred)[(y_true == 0) & (groups == 1)])\n",
    "    EO=np.sum([abs(white_tpr_1-black_tpr_1),abs(white_tpr_0-black_tpr_0)])\n",
    "    if EO<1.01:\n",
    "        return EO\n",
    "    else:\n",
    "        return 0\n",
    "def compute_accuracy(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred)\n",
    "for state in states:\n",
    "    y_val_pred[state]=global_model.predict(val_features[state])\n",
    "    y_val_pred[state]=np.where(y_val_pred[state]<0.5,0,1)\n",
    "    EO=compute_equal_odd(val_labels[state],y_val_pred[state],vali_groups[state])\n",
    "    acc=compute_accuracy(val_labels[state],y_val_pred[state])\n",
    "    local_acc.append(acc)\n",
    "    local_EO.append(EO)\n",
    "all_valid_pred=global_model.predict(all_vaild_features)\n",
    "all_valid_pred=np.where(all_valid_pred<0.5,0,1)\n",
    "global_EO=compute_equal_odd(all_vaild_labels,all_valid_pred,all_valid_groups)\n",
    "print('global EO',global_EO)\n",
    "print('local EO',local_EO)\n",
    "local_EO_avg=np.mean(local_EO)\n",
    "print('local EO average',local_EO_avg)\n",
    "acc_diff_1=abs(max(local_acc)-min(local_acc))\n",
    "print('acc diff:',acc_diff_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5e5c220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10532/10532 [==============================] - 5s 488us/step - loss: 0.4907 - accuracy: 0.7743\n",
      "loss, accuracy: [0.4906599521636963, 0.7742937803268433]\n"
     ]
    }
   ],
   "source": [
    "acc=global_model.evaluate(all_vaild_features,all_vaild_labels)\n",
    "print('loss, accuracy:',acc)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ef7bb01b592f75",
   "metadata": {},
   "source": [
    "# Compute the local statistics for LP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9059f59f37de6ebb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T18:31:05.338315Z",
     "start_time": "2024-09-04T18:31:05.335665Z"
    }
   },
   "outputs": [],
   "source": [
    "states_for_local_fairness = [\n",
    "    \"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DE\", \"FL\", \"GA\",\n",
    "    \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\",\n",
    "    \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\",\n",
    "    \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\",\n",
    "    \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c1e6e8aa2ce0f97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T18:31:05.392707Z",
     "start_time": "2024-09-04T18:31:05.383814Z"
    }
   },
   "outputs": [],
   "source": [
    "# def p_computation(label, sensitive_attribute, y, a):\n",
    "#     label=np.reshape(label,(-1,))\n",
    "#     N=len(label)\n",
    "#     sensitive_attribute=np.reshape(sensitive_attribute,(-1,))# Total number of samples\n",
    "#     # Create a boolean mask for where both conditions are met\n",
    "#     mask = (label == y) & (sensitive_attribute == a)\n",
    "#     # print(mask)\n",
    "#     # print(\"Number of matching samples:\", np.sum(mask))\n",
    "    \n",
    "#     # Compute the probability\n",
    "#     p_y_ac = np.sum(mask) / N\n",
    "#     return p_y_ac\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def p_computation(label, sensitive_attribute, y, a):\n",
    "    \"\"\"\n",
    "    Computes a differentially private estimate of P(y, a | c)\n",
    "    by adding Laplace noise.\n",
    "\n",
    "    Parameters:\n",
    "    - label: array-like, predicted or actual labels\n",
    "    - sensitive_attribute: array-like, sensitive attributes (e.g., gender, race)\n",
    "    - y: target label value to compute probability for\n",
    "    - a: target sensitive attribute value\n",
    "    - epsilon: privacy budget (smaller = more privacy, more noise)\n",
    "\n",
    "    Returns:\n",
    "    - p_y_ac_noisy: Noisy probability estimate\n",
    "    \"\"\"\n",
    "    epsilon = 0.01  # Privacy budg\n",
    "    label = np.reshape(label, (-1,))\n",
    "    sensitive_attribute = np.reshape(sensitive_attribute, (-1,))\n",
    "    N = len(label)\n",
    "\n",
    "    # Compute true probability\n",
    "    mask = (label == y) & (sensitive_attribute == a)\n",
    "    p_y_ac = np.sum(mask) / N\n",
    "\n",
    "    # Sensitivity is 1/N for counting queries normalized by N\n",
    "    sensitivity = 1 / N\n",
    "\n",
    "    # Sample Laplace noise\n",
    "    noise = np.random.laplace(loc=0.0, scale=sensitivity / epsilon)\n",
    "\n",
    "    # Add noise and ensure result is clipped to [0,1]\n",
    "    p_y_ac_noisy = np.clip(p_y_ac + noise, 0, 1)\n",
    "\n",
    "    return p_y_ac_noisy\n",
    "#community_1\n",
    "S= np.zeros((len(states),2,2))\n",
    "L=np.zeros((len(states),2,2))\n",
    "p = np.zeros(len(states))\n",
    "l=[]\n",
    "for c,state in enumerate(states):\n",
    "    p[c]=len(val_features[state])/len(all_vaild_features)\n",
    "for c,state in enumerate(states):\n",
    "    for a in range(2):\n",
    "        for y in range(2):\n",
    "            S[c,a,y] =p[c]* p_computation(val_labels[state],vali_groups[state], y, a)\n",
    "\n",
    "            L[c,a,y] =p_computation(val_labels[state],vali_groups[state], y, a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a06f151e8e0014",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T18:31:13.659177Z",
     "start_time": "2024-09-04T18:31:05.422582Z"
    }
   },
   "outputs": [],
   "source": [
    "TP= np.zeros((len(states),2,2))\n",
    "def compute_TP (y_pred,label, sensitive_attribute, y, a):\n",
    "    y_pred=np.reshape(y_pred,(-1,))\n",
    "    label=np.reshape(label,(-1,))\n",
    "    N=len(y_pred)\n",
    "    sensitive_attribute=np.reshape(sensitive_attribute,(-1,))# Total number of samples\n",
    "    # Create a boolean mask for where both conditions are met\n",
    "    count_1= (y_pred == y) & (label == y) & (sensitive_attribute == a)\n",
    "    count_2 = (label== y) & (sensitive_attribute == a)\n",
    "    # print(sum(count_1),sum(count_2))\n",
    "\n",
    "    \n",
    "    # Compute the probability\n",
    "    TP_y_ac = np.sum(count_1) / np.sum(count_2)\n",
    "    return TP_y_ac\n",
    "\n",
    "for c, state in enumerate(states):\n",
    "    y_pred=global_model.predict(val_features[state])\n",
    "    y_pred=np.where(y_pred<0.5,0,1)\n",
    "    for a in range(2):\n",
    "        for y in range(2):\n",
    "            TP[c,a,y]=compute_TP(y_pred,val_labels[state],vali_groups[state],y,a)\n",
    "# print(TP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca12b83f091d5a25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T18:31:13.708073Z",
     "start_time": "2024-09-04T18:31:13.702905Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_alpha(label, sensitive_attribute, y, a):\n",
    "    label=np.reshape(label,(-1,))\n",
    "    N=len(label)\n",
    "    sensitive_attribute=np.reshape(sensitive_attribute,(-1,))# Total number of samples\n",
    "    # Create a boolean mask for where both conditions are met\n",
    "    mask = (label == y) & (sensitive_attribute == a)\n",
    "    # print(mask)\n",
    "    # print(\"Number of matching samples:\", np.sum(mask))\n",
    "    \n",
    "    # Compute the probability\n",
    "    alpha_y_ac = np.sum(mask) / N\n",
    "    return alpha_y_ac\n",
    "alpha_a_y = np.zeros((2,2))\n",
    "for a in range(2):\n",
    "    for y in range(2):\n",
    "        alpha_a_y[a,y]=compute_alpha(all_vaild_labels,all_valid_groups,y,a)\n",
    "# print(alpha_a_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4108092",
   "metadata": {},
   "source": [
    "## Solve the LP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef269116d7a3977",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T20:21:46.144476Z",
     "start_time": "2024-09-04T20:21:45.621783Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import linprog\n",
    "import numpy as np\n",
    "def LP_EO(e_g,e_l,e_c):\n",
    "# define the objective function\n",
    "    C=[]\n",
    "    for c in range(len(states)):\n",
    "        for a in range(2):\n",
    "            for y in range(2):\n",
    "                C.append(-S[c,a,y])\n",
    "\n",
    "\n",
    "\n",
    "    #define the global fairness constraints\n",
    "    N=2\n",
    "    K=2\n",
    "    A_1=[]\n",
    "    A_2=[]\n",
    "    for c in range(len(states)):\n",
    "        for a in range(2):\n",
    "            for y in range(2):\n",
    "                if  a==0 and y==0:\n",
    "                    A_1.append(-S[c,a,y]/alpha_a_y[a,y])\n",
    "                    # print(S[c,a,y],alpha_a_y[a,y])\n",
    "                elif  a==1 and y==0:\n",
    "                    A_1.append(S[c,a,y]/alpha_a_y[a,y])\n",
    "    \n",
    "                else:\n",
    "                    A_1.append(0)\n",
    "    \n",
    "    for c in range(len(states)):\n",
    "        for a in range(2):\n",
    "            for y in range(2):\n",
    "                if c==c and a==0 and y==1:\n",
    "                    A_2.append(-S[c,a,y]/alpha_a_y[a,y])\n",
    "                elif c==c and a==1 and y==1:\n",
    "                    A_2.append(S[c,a,y]/alpha_a_y[a,y])\n",
    "                else:\n",
    "                    A_2.append(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # define local fairness constraints\n",
    "     # Identity matrix of size 50x50, where each row is a basis vector\n",
    "    A_1=np.reshape(A_1,(1,len(states)*2*2))\n",
    "    A_2=np.reshape(A_2,(1,len(states)*2*2))\n",
    "    # print(np.shape(A_1),np.shape(A_2))\n",
    "    # To access e_i, you can use basis_vectors[i], e.g., e_0 is basis_vectors[0]\n",
    "    \n",
    "    # Define zero vector in R^50\n",
    "    zero_vec = np.zeros((2,4))\n",
    "    I_vector =np.eye(2)\n",
    "    basis_vectors = np.hstack([I_vector, -I_vector])\n",
    "    \n",
    "    # Construct the matrix\n",
    "    # First row: [e_0, e_1, 0, 0]\n",
    "    rows = []\n",
    "  \n",
    "# Construct 50 rows dynamically\n",
    "    for i in range(len(states)):\n",
    "        # Initialize the blocks for this row\n",
    "        row_blocks = []\n",
    "        \n",
    "        for j in range(len(states)):\n",
    "            if i == j:  # Diagonal pattern with identity matrices\n",
    "                row_blocks.append(basis_vectors)\n",
    "            else:  # All other positions are zero matrices\n",
    "                row_blocks.append(zero_vec)\n",
    "        \n",
    "        # Horizontally stack the blocks for the current row\n",
    "        row = np.hstack(row_blocks)\n",
    "        rows.append(row)\n",
    "    \n",
    "    # Combine all rows into a single matrix\n",
    "    A_3 = np.vstack(rows)\n",
    "    # print(A_3)\n",
    "\n",
    "\n",
    "    ### client fairness constraints\n",
    "    rows_1=[]\n",
    "    for i in range(len(states)):\n",
    "        row_blocks_1 = []\n",
    "        for j in range(len(states)):\n",
    "            if i == j:\n",
    "                for a in range(2):\n",
    "                    for y in range(2):\n",
    "                        row_blocks_1.append(-((len(states)-1)/len(states))*L[j,a,y])\n",
    "            else:\n",
    "                for a in range(2):\n",
    "                    for y in range(2):\n",
    "                        row_blocks_1.append((1/len(states))*L[j,a,y])\n",
    "        row_1=np.hstack(row_blocks_1)\n",
    "        rows_1.append(row_1)\n",
    "\n",
    "    A_4=np.vstack(rows_1)\n",
    "    print(np.shape(A_3))\n",
    "    print(np.shape(A_4))     \n",
    "\n",
    "    # Combine rows into a single matrix\n",
    "    # define the property of dervied outcome predictor\n",
    "    def K_ac_compute(a,c):\n",
    "        K_ac=np.zeros((3,2))\n",
    "        l_ac=np.zeros((3,1))\n",
    "        K_ac[0:]=[-1,-1]\n",
    "        K_ac[1:]=[(1-TP[c,a,1]), TP[c,a,0]]\n",
    "        K_ac[2:]=[TP[c,a,1], (1-TP[c,a,0])]\n",
    "        l_ac[0:]=[-1]\n",
    "        l_ac[1:]=[TP[c,a,0]]\n",
    "        l_ac[2:]=[TP[c,a,1]]\n",
    "        return K_ac,l_ac\n",
    "    num_clients = len(states)\n",
    "    block_rows = 3\n",
    "    block_cols = 2\n",
    "    # Define the submatrices k_01, k_11, k_02, k_12 as 3x2 matrices\n",
    "    M = np.zeros((3*2*len(states),2*2*len(states)))\n",
    "    l = np.zeros((3*2*len(states), 1))\n",
    "    \n",
    "    # Construct the matrices for all clients\n",
    "    for c in range(num_clients):\n",
    "        for a in range(2):\n",
    "            # Compute submatrices for client c and attribute a\n",
    "            K_ac, l_ac = K_ac_compute(a, c)\n",
    "            \n",
    "            # Calculate starting positions for placing K_ac in M\n",
    "            start_row = (2 * c + a) * block_rows\n",
    "            start_col = (2 * c + a) * block_cols\n",
    "            \n",
    "            # Place K_ac and l_ac in the appropriate block of M and l\n",
    "            M[start_row:start_row + block_rows, start_col:start_col + block_cols] = K_ac\n",
    "            l[start_row:start_row + block_rows] = l_ac\n",
    "    \n",
    "    # Verify the dimensions of the resulting matrix and vector\n",
    "    # print(\"Shape of matrix M:\", M.shape)\n",
    "    # print(\"Shape of vector l:\", l.shape)\n",
    "    # print(np.shape(A_1),np.shape(A_2),np.shape(A_3),np.shape(M))\n",
    "    # combine the constraints\n",
    "    print(np.shape(A_1),np.shape(A_2),np.shape(A_3),np.shape(M))\n",
    "    A=np.vstack((A_1,A_2,A_3, A_4,-A_1, -A_2, -A_3,-A_4,M))\n",
    "    print(np.shape(A))\n",
    "\n",
    "    b_global=e_g*np.ones((1,1))\n",
    "    b_local=e_l*np.ones((len(states)*2,1))\n",
    "    b_client=e_c*np.ones((len(states),1))\n",
    "    # print(b_local)\n",
    "    b=np.vstack((b_global,b_global, b_local,b_client,b_global,b_global, b_local,b_client,l))\n",
    "    print(np.shape(b))  \n",
    "    res = linprog(C, A_ub=A, b_ub=b)\n",
    "    x=res.x\n",
    "    print(np.shape(x))\n",
    "    x=np.reshape(x,(len(states),2,2))\n",
    "    print(A_4)\n",
    "    \n",
    "    return x,res.fun\n",
    "x,fun=LP_EO(0.01,0.01,0.02)\n",
    "# print('objective function:',fun)  \n",
    "# acc_client=[]             \n",
    "# for i in range (len(states)):\n",
    "#     acc=0\n",
    "#     for j in range(2):\n",
    "#         for k in range(2):\n",
    "#             acc+=x[i,j,k]*L[i,j,k]\n",
    "#     acc_client.append(acc)\n",
    "# print('acc_client:',acc_client)\n",
    "# print(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff19a88a1f3861",
   "metadata": {},
   "source": [
    "# Clients solve the LAE and generate fair outcome prediction\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2310a969fda69cb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T19:40:14.152557Z",
     "start_time": "2024-09-04T19:40:08.079377Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116/116 [==============================] - 0s 481us/step\n",
      "15/15 [==============================] - 0s 485us/step\n",
      "157/157 [==============================] - 0s 470us/step\n",
      " 1/74 [..............................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4043572/3366997340.py:49: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  y_hat=int(y_pred[i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 0s 452us/step\n",
      "866/866 [==============================] - 0s 446us/step\n",
      "111/111 [==============================] - 0s 475us/step\n",
      "71/71 [==============================] - 0s 465us/step\n",
      "18/18 [==============================] - 0s 490us/step\n",
      "446/446 [==============================] - 0s 459us/step\n",
      "236/236 [==============================] - 0s 437us/step\n",
      "30/30 [==============================] - 0s 454us/step\n",
      "38/38 [==============================] - 0s 454us/step\n",
      "268/268 [==============================] - 0s 435us/step\n",
      "153/153 [==============================] - 0s 438us/step\n",
      "65/65 [==============================] - 0s 450us/step\n",
      "65/65 [==============================] - 0s 439us/step\n",
      "108/108 [==============================] - 0s 439us/step\n",
      "106/106 [==============================] - 0s 438us/step\n",
      "29/29 [==============================] - 0s 489us/step\n",
      "113/113 [==============================] - 0s 474us/step\n",
      "138/138 [==============================] - 0s 474us/step\n",
      "223/223 [==============================] - 0s 438us/step\n",
      "103/103 [==============================] - 0s 464us/step\n",
      "74/74 [==============================] - 0s 456us/step\n",
      "140/140 [==============================] - 0s 463us/step\n",
      "22/22 [==============================] - 0s 486us/step\n",
      "40/40 [==============================] - 0s 466us/step\n",
      "64/64 [==============================] - 0s 456us/step\n",
      "27/27 [==============================] - 0s 468us/step\n",
      "173/173 [==============================] - 0s 444us/step\n",
      "49/49 [==============================] - 0s 449us/step\n",
      "423/423 [==============================] - 0s 437us/step\n",
      "233/233 [==============================] - 0s 458us/step\n",
      "15/15 [==============================] - 0s 509us/step\n",
      "258/258 [==============================] - 0s 456us/step\n",
      "93/93 [==============================] - 0s 433us/step\n",
      "90/90 [==============================] - 0s 444us/step\n",
      "276/276 [==============================] - 0s 443us/step\n",
      "21/21 [==============================] - 0s 489us/step\n",
      "117/117 [==============================] - 0s 447us/step\n",
      "18/18 [==============================] - 0s 478us/step\n",
      "160/160 [==============================] - 0s 462us/step\n",
      "619/619 [==============================] - 0s 432us/step\n",
      "73/73 [==============================] - 0s 441us/step\n",
      "13/13 [==============================] - 0s 505us/step\n",
      "172/172 [==============================] - 0s 439us/step\n",
      "152/152 [==============================] - 0s 448us/step\n",
      "45/45 [==============================] - 0s 463us/step\n",
      "117/117 [==============================] - 0s 452us/step\n",
      "12/12 [==============================] - 0s 529us/step\n",
      "0.388351509404141\n"
     ]
    }
   ],
   "source": [
    "beta=np.zeros((50,2,3))\n",
    "for a in range(2):\n",
    "        for c in range(50):\n",
    "            A=np.array([[1,1,1],[TP[c,a,0],1,0],[TP[c,a,1],0,1]])\n",
    "            b=np.array([1,x[c,a,0],x[c,a,1]])\n",
    "            beta_ac=np.linalg.solve(A,b)\n",
    "            beta[c,a,:]=beta_ac\n",
    "\n",
    "def compute_tilde_Y(c,a, Y_hat, y_values):\n",
    "    \"\"\"\n",
    "    Compute \\widetilde{Y}_{\\boldsymbol{\\beta}_{ac}}(x,a,c) based on given probabilities.\n",
    "\n",
    "    Parameters:\n",
    "    - beta_ac: Dictionary with keys 'beta_0' and 'beta_y' for probabilities.\n",
    "    - Y_hat: The predicted value \\hat{Y}(x,a,c).\n",
    "    - y_values: List or array of possible y values in \\mathcal{Y}.\n",
    "\n",
    "    Returns:\n",
    "    - tilde_Y: The computed value of \\widetilde{Y}.\n",
    "    \"\"\"\n",
    "    # Extract probabilities\n",
    "    beta_0 = beta[c, a, 0]  # Probability for Y_hat\n",
    "    beta_y = beta[c, a, 1:] \n",
    "    # Probabilities for other y in \\mathcal{Y}\n",
    "    \n",
    "    # Normalize probabilities\n",
    "    \n",
    "    # Generate a random number\n",
    "    rand_val = np.random.rand()\n",
    "    \n",
    "    # Determine the output based on random value\n",
    "    if rand_val < beta_0:\n",
    "        return Y_hat\n",
    "    else:\n",
    "        cumulative_prob = beta_0\n",
    "        for y in y_values:\n",
    "            cumulative_prob += beta_y[y]\n",
    "            if rand_val < cumulative_prob:\n",
    "                return y\n",
    "y_tilde_list = [[] for _ in range(50)] \n",
    "# Example usage\n",
    "for c, state in enumerate(states):\n",
    "    client_features = test_features[state]\n",
    "    client_sensitive = test_groups[state]\n",
    "    y_pred=global_model.predict(test_features[state])\n",
    "    y_pred=np.where(y_pred<0.5,0,1)\n",
    "    for i in range (len(y_pred)):\n",
    "        a=int(client_sensitive[i])\n",
    "        y_hat=int(y_pred[i])\n",
    "        y_tilde=compute_tilde_Y(c,a,y_hat,[0,1])\n",
    "        y_tilde=int(y_tilde)\n",
    "        y_tilde_list[c].append(y_tilde)\n",
    "# print(y_tilde_list[1])\n",
    "acc_list=[]\n",
    "for c, state in enumerate(states):\n",
    "    y_tilde=np.array(y_tilde_list[c])\n",
    "    acc=compute_accuracy(test_labels[state],y_tilde)\n",
    "    acc_list.append(acc)\n",
    "acc_diff=abs(max(acc_list)-min(acc_list))\n",
    "print(acc_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93273c2",
   "metadata": {},
   "source": [
    "## Fairness and Accuracy after post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "223403d0e9dcdfbf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T19:40:31.646239Z",
     "start_time": "2024-09-04T19:40:31.601599Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local post EO [0.030086919806439294, 0.08386782408599935, 0.031545117317950844, 0.050858266244310446, 0.034310255867296635, 0.08943677007484113, 0.06855530552604688, 0.02876208361361715, 0.01902609718678061, 0.05677448106303995, 0.12142674868990655, 0.10072248029007824, 0.029357581852720138, 0.040456345476454214, 0.04633528924456057, 0.12122043861795623, 0.028060167527992508, 0.04722254540094534, 0.0, 0.030689616158052657, 0.033118571514147654, 0.021692281397528212, 0.05956833418438212, 0.019850079530373543, 0.0008123362797156641, 0.1604081423145175, 0.07674186180984849, 0.007712223736933088, 0.22863109236882656, 0.07492171077748916, 0.056845445039375275, 0.03247390962143358, 0.053951018160315334, 0.08974934025056969, 0.053314891180647694, 0.031451001282074564, 0.05828227442861089, 0.014932672857041696, 0.17286619075131954, 0.055554736722200904, 0.13737427223034238, 0.11059940003704843, 0.04335306190397614, 0.10417421223692219, 0.1683973656865223, 0.03215559824445069, 0.033570697095112734, 0.20144537368538828, 0.008741006166412157, 0.17658064158064157]\n",
      "local EO 0.06755968154238313\n",
      "global EO 0.01732861871667074\n",
      "accuracy 0.5752418759067565\n"
     ]
    }
   ],
   "source": [
    "local_post_EO=[]\n",
    "for c,state in enumerate(states):\n",
    "    y_post=y_tilde_list[c]\n",
    "    y_post=np.reshape(y_post,(-1,))\n",
    "    y_true=test_labels[state]\n",
    "    y_group=test_groups[state]\n",
    "    dis=compute_equal_odd(y_true,y_post,y_group)\n",
    "    local_post_EO.append(dis)\n",
    "print('local post EO',local_post_EO)\n",
    "local_EO=np.mean(local_post_EO)\n",
    "print('local EO',local_EO)\n",
    "y_tilde_list = np.concatenate(y_tilde_list)  \n",
    "\n",
    "post_EO=compute_equal_odd(all_test_labels,y_tilde_list,all_test_groups)\n",
    "print('global EO', post_EO)\n",
    "#compute accuracy\n",
    "count=np.where(y_tilde_list==all_test_labels,1,0)\n",
    "accuracy=np.sum(count)/len(count)\n",
    "print('accuracy',accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9279441a",
   "metadata": {},
   "source": [
    "## Print the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dbf10d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before post processing\n",
      "local equal opportunity 0.09219603835592173\n",
      "global equal opportunity 0.05928586036608663\n",
      "accuracy 0.35789473684210527\n",
      "acc diff: 0.2721233585930334\n",
      "after post processing\n",
      "local equal opportunity 0.06755968154238313\n",
      "global equal opportunity 0.01732861871667074\n",
      "accuracy 0.5752418759067565\n",
      "acc_diff 0.388351509404141\n"
     ]
    }
   ],
   "source": [
    "print('before post processing')\n",
    "print('local equal opportunity',local_EO_avg)\n",
    "print('global equal opportunity',global_EO)\n",
    "print('accuracy',acc)\n",
    "print('acc diff:',acc_diff_1)\n",
    "print('after post processing')\n",
    "print('local equal opportunity',local_EO)\n",
    "print('global equal opportunity',post_EO)\n",
    "print('accuracy',accuracy)\n",
    "print('acc_diff', acc_diff)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

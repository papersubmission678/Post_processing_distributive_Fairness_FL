{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9112e490d5cdc821",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T22:17:02.364156Z",
     "start_time": "2024-09-09T22:17:02.358218Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "IMAGE_AUGMENTATION=True\n",
    "AUGMENTATION_RATIO=1\n",
    "batch_size=32\n",
    "E=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T22:17:05.846940Z",
     "start_time": "2024-09-09T22:17:02.388182Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 16:03:55.704170: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-11 16:03:55.726639: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-05-11 16:03:55.726657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-11 16:03:55.726672: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-11 16:03:55.731047: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-11 16:03:55.731695: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-11 16:03:56.420616: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.applications import DenseNet121,VGG16\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from data_processing import data_processing\n",
    "from data_2 import data_simulate_noniid\n",
    "# from HM_10000_non_i_i_d import data_processing\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b671256bc0b3aa80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T22:17:07.329796Z",
     "start_time": "2024-09-09T22:17:05.886167Z"
    }
   },
   "outputs": [],
   "source": [
    "# train_x,train_y,train_s, vali_x, vali_y, vali_s, train_x_1, vali_x_1, train_y_1, vali_y_1, train_s_1, vali_s_1, train_x_2, vali_x_2, train_y_2, vali_y_2, train_s_2, vali_s_2, train_x_3, vali_x_3, train_y_3, vali_y_3, train_s_3, vali_s_3, train_x_4, vali_x_4, train_y_4, vali_y_4, train_s_4, vali_s_4, train_x_5, vali_x_5, train_y_5, vali_y_5, train_s_5, vali_s_5=data_processing()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d8c1df04125f5d",
   "metadata": {},
   "source": [
    "Prepare training data, data_augumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "417e80bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1, 0.3, 0.5, 0.7, 0.9]\n",
      "(750, 2352) (750, 2352) (750, 2352) (750, 2352) (749, 2352)\n"
     ]
    }
   ],
   "source": [
    "train_x,train_y,train_s, vali_x, vali_y, vali_s, train_x_1, vali_x_1, train_y_1, vali_y_1, train_s_1, vali_s_1, train_x_2, vali_x_2, train_y_2, vali_y_2, train_s_2, vali_s_2, train_x_3, vali_x_3, train_y_3, vali_y_3, train_s_3, vali_s_3, train_x_4, vali_x_4, train_y_4, vali_y_4, train_s_4, vali_s_4, train_x_5, vali_x_5, train_y_5, vali_y_5, train_s_5, vali_s_5=data_simulate_noniid(settings=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebebd9871fc9b87f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T22:17:07.380937Z",
     "start_time": "2024-09-09T22:17:07.376764Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_dataset(dataset, is_training):\n",
    "    dataset = dataset.cache().shuffle(10, reshuffle_each_iteration = False)\n",
    "    \n",
    "    if is_training == True and IMAGE_AUGMENTATION == True:\n",
    "        print(\"Images in training dataset before augmentation: \" + str(len(dataset)))\n",
    "        dataset_augmented = dataset.take(int(AUGMENTATION_RATIO*len(dataset))).map(augment, num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
    "        dataset = dataset.concatenate(dataset_augmented)\n",
    "        print(\"Images in training dataset after augmentation: \" + str(len(dataset)))\n",
    "\n",
    "    dataset = dataset.batch(batch_size).prefetch(buffer_size = tf.data.AUTOTUNE)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def augment(image, label_y):\n",
    "    # image = tf.image.rotate(image, random.uniform(-10, 10)*math.pi/180)\n",
    "    image = tf.image.central_crop(image, random.uniform(0.9, 1.0))\n",
    "    image = tf.image.random_brightness(image, max_delta = 0.1)\n",
    "    image = tf.image.random_contrast(image, lower = 0.9, upper = 1.1)\n",
    "    image = tf.image.resize(image, [28, 28])\n",
    "    \n",
    "    return image, label_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfec5f381447dddd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T22:17:07.425470Z",
     "start_time": "2024-09-09T22:17:07.422915Z"
    }
   },
   "outputs": [],
   "source": [
    "def data_augmentation(train_x,train_y):\n",
    "    training_set = tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
    "    training_set = preprocess_dataset(training_set, is_training = True)\n",
    "    return training_set\n",
    "# def data_augmentation(train_x, train_y):\n",
    "#     dataset=tf.data.Dataset.from_tensor_slices((train_x, train_y)).batch(batch_size=batch_size)\n",
    "#     return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c089d78db6823723",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T22:17:11.030170Z",
     "start_time": "2024-09-09T22:17:07.449111Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images in training dataset before augmentation: 750\n",
      "Images in training dataset after augmentation: 1500\n",
      "Images in training dataset before augmentation: 750\n",
      "Images in training dataset after augmentation: 1500\n",
      "Images in training dataset before augmentation: 750\n",
      "Images in training dataset after augmentation: 1500\n",
      "Images in training dataset before augmentation: 750\n",
      "Images in training dataset after augmentation: 1500\n",
      "Images in training dataset before augmentation: 749\n",
      "Images in training dataset after augmentation: 1498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 16:03:58.822848: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-11 16:03:58.841037: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2211] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "training_set_1=data_augmentation(train_x_1, train_y_1)\n",
    "training_set_2=data_augmentation(train_x_2, train_y_2)\n",
    "training_set_3=data_augmentation(train_x_3, train_y_3)\n",
    "training_set_4=data_augmentation(train_x_4, train_y_4)\n",
    "training_set_5=data_augmentation(train_x_5, train_y_5)\n",
    "p_1=len(training_set_1)/(len(training_set_1)+len(training_set_2)+len(training_set_3)+len(training_set_4)+len(training_set_5))\n",
    "p_2=len(training_set_2)/(len(training_set_1)+len(training_set_2)+len(training_set_3)+len(training_set_4)+len(training_set_5))\n",
    "p_3=len(training_set_3)/(len(training_set_1)+len(training_set_2)+len(training_set_3)+len(training_set_4)+len(training_set_5))\n",
    "p_4=len(training_set_4)/(len(training_set_1)+len(training_set_2)+len(training_set_3)+len(training_set_4)+len(training_set_5))\n",
    "p_5=len(training_set_5)/(len(training_set_1)+len(training_set_2)+len(training_set_3)+len(training_set_4)+len(training_set_5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6e4fe642bf2ff2",
   "metadata": {},
   "source": [
    "FedAvg algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a19aa3b656a7708d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T22:17:11.060357Z",
     "start_time": "2024-09-09T22:17:11.052970Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense, MaxPool2D, Input, MaxPooling2D, GlobalAveragePooling2D, Dropout\n",
    "import tensorflow as tf\n",
    "def create_CNN_model(): \n",
    "    model = Sequential([\n",
    "        Input(shape=(28, 28, 3)),\n",
    "        Conv2D(32, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(4, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "# def create_CNN_model():\n",
    "#     model = Sequential()\n",
    "# \n",
    "#     model.add(Conv2D(16, \n",
    "#                      kernel_size = (3,3), \n",
    "#                      input_shape = (28, 28, 3), \n",
    "#                      activation = 'relu', \n",
    "#                      padding = 'same'))\n",
    "#     \n",
    "#     model.add(MaxPool2D(pool_size = (2,2)))\n",
    "#     model.add(tf.keras.layers.BatchNormalization())\n",
    "#     \n",
    "#     model.add(Conv2D(32, \n",
    "#                      kernel_size = (3,3), \n",
    "#                      activation = 'relu'))\n",
    "#     \n",
    "#     model.add(Conv2D(64, \n",
    "#                      kernel_size = (3,3), \n",
    "#                      activation = 'relu'))\n",
    "#     \n",
    "#     model.add(MaxPool2D(pool_size = (2,2)))\n",
    "#     \n",
    "#     model.add(tf.keras.layers.BatchNormalization())\n",
    "#     \n",
    "#     model.add(Conv2D(128, \n",
    "#                      kernel_size = (3,3), \n",
    "#                      activation = 'relu'))\n",
    "#     \n",
    "#     model.add(Conv2D(256, \n",
    "#                      kernel_size = (3,3), \n",
    "#                      activation = 'relu'))\n",
    "#     \n",
    "#     model.add(Flatten())\n",
    "#     model.add(tf.keras.layers.Dropout(0.2))\n",
    "#     model.add(Dense(256,activation='relu'))\n",
    "#     \n",
    "#     model.add(tf.keras.layers.BatchNormalization())\n",
    "#     model.add(tf.keras.layers.Dropout(0.2))\n",
    "#     model.add(Dense(128,activation='relu'))\n",
    "#     \n",
    "#     model.add(tf.keras.layers.BatchNormalization())\n",
    "#     model.add(Dense(64,activation='relu'))\n",
    "#     \n",
    "#     model.add(tf.keras.layers.BatchNormalization())\n",
    "#     model.add(tf.keras.layers.Dropout(0.2))\n",
    "#     model.add(Dense(32,activation='relu'))\n",
    "#     \n",
    "#     model.add(tf.keras.layers.BatchNormalization())\n",
    "#     model.add(Dense(7,activation='softmax'))\n",
    "#     \n",
    "#     model.summary()\n",
    "#     return model\n",
    "def multiply(weights,c):\n",
    "\n",
    "    new_weights = []\n",
    "    for i in range(len(weights)):\n",
    "        new_weights.append(weights[i]*c)\n",
    "    return new_weights\n",
    "\n",
    "# def norm_square(weights_n):\n",
    "#     sum = 0\n",
    "#     for i in range(len(weights_n)):\n",
    "#         sum += np.linalg.norm(weights_n[i])**2\n",
    "#     return sum\n",
    "def model_weight_aggregation(weight_1, weight_2, weight_3, weight_4, weight_5):\n",
    "    added_weight = []\n",
    "    weight_1=multiply(weight_1,p_1)\n",
    "    weight_2=multiply(weight_2,p_2)\n",
    "    weight_3=multiply(weight_3,p_3)\n",
    "    weight_4=multiply(weight_4,p_4)\n",
    "    weight_5=multiply(weight_5,p_5)\n",
    "    for i in range(len(weight_1)):\n",
    "        added_weight.append(weight_1[i]+weight_2[i]+weight_3[i]+weight_4[i]+weight_5[i])\n",
    "    return added_weight\n",
    "def model_weight_minus(weight_1, weight_2):\n",
    "\n",
    "    minus_weights = []\n",
    "    for i in range(len(weight_1)):\n",
    "        minus_weights.append(weight_1[i] - weight_2[i])\n",
    "    return minus_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e25a15b564fb4be1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T22:17:11.140373Z",
     "start_time": "2024-09-09T22:17:11.082453Z"
    }
   },
   "outputs": [],
   "source": [
    "clients_model=create_CNN_model()\n",
    "w_1=clients_model.get_weights()\n",
    "w_2=clients_model.get_weights()\n",
    "w_3=clients_model.get_weights()\n",
    "w_4=clients_model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32133062c35a803c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T22:17:11.207238Z",
     "start_time": "2024-09-09T22:17:11.174060Z"
    }
   },
   "outputs": [],
   "source": [
    "global_model=create_CNN_model()\n",
    "w_0=global_model.get_weights()\n",
    "global_weight=[]\n",
    "global_weight.append(w_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "313566a665d5d23c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T22:17:11.249204Z",
     "start_time": "2024-09-09T22:17:11.246152Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(training_set,loacl_epoch):\n",
    "    w_t=global_weight[0]\n",
    "    clients_model.set_weights(w_t)\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "    clients_model.compile(loss = 'sparse_categorical_crossentropy',\n",
    "                  optimizer =optimizer,\n",
    "                  metrics = ['accuracy'])\n",
    "    history =clients_model.fit(training_set,\n",
    "                    epochs = 5)\n",
    "    return clients_model.get_weights()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "228fef50299c882",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T22:17:11.326643Z",
     "start_time": "2024-09-09T22:17:11.323815Z"
    }
   },
   "outputs": [],
   "source": [
    "def Fedavg():\n",
    "    w_t_1=train_model(training_set_1,E)\n",
    "    w_t_2=train_model(training_set_2,E)\n",
    "    w_t_3=train_model(training_set_3,E)\n",
    "    w_t_4=train_model(training_set_4,E)\n",
    "    w_t_5=train_model(training_set_5,E)\n",
    "    new_w=model_weight_aggregation(w_t_1,w_t_2,w_t_3,w_t_4,w_t_5)\n",
    "    global_weight[0]=new_w\n",
    "    global_model.set_weights(new_w)\n",
    "    return new_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7be5e6b70fcc7fcc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T22:17:11.336674Z",
     "start_time": "2024-09-09T22:17:11.334084Z"
    }
   },
   "outputs": [],
   "source": [
    "def acc_calculator(y_true, y_pred):\n",
    "    accuracy_count=np.where(y_true==y_pred, 1.0, 0.0)\n",
    "    accuracy=sum(accuracy_count)/len(y_true)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1a54bae2d126ea8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T22:20:40.490215Z",
     "start_time": "2024-09-09T22:17:11.375129Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.8820 - accuracy: 0.7067\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.7469 - accuracy: 0.7067\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.7311 - accuracy: 0.7067\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.7258 - accuracy: 0.7067\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.7153 - accuracy: 0.7067\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.9116 - accuracy: 0.6827\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.7929 - accuracy: 0.6827\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.7824 - accuracy: 0.6827\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.7677 - accuracy: 0.6827\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.7627 - accuracy: 0.6827\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.9469 - accuracy: 0.6587\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.8221 - accuracy: 0.6587\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.8113 - accuracy: 0.6587\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.7995 - accuracy: 0.6587\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.7895 - accuracy: 0.6587\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.9376 - accuracy: 0.6587\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.8335 - accuracy: 0.6587\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.8121 - accuracy: 0.6587\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.8005 - accuracy: 0.6587\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.7990 - accuracy: 0.6587\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.9447 - accuracy: 0.6595\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.8207 - accuracy: 0.6595\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.7912 - accuracy: 0.6595\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.7786 - accuracy: 0.6595\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.7688 - accuracy: 0.6595\n",
      "8.175230026245117\n",
      "vali_acc= 0.6738666666666666\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.7299 - accuracy: 0.7067\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.7127 - accuracy: 0.7067\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.6928 - accuracy: 0.7093\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.6798 - accuracy: 0.7193\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.6701 - accuracy: 0.7253\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.7775 - accuracy: 0.6827\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.7548 - accuracy: 0.6827\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.7417 - accuracy: 0.6820\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.7207 - accuracy: 0.6940\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.7182 - accuracy: 0.6867\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.8080 - accuracy: 0.6587\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.7861 - accuracy: 0.6587\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.7761 - accuracy: 0.6593\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.7641 - accuracy: 0.6633\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.7475 - accuracy: 0.6627\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.8059 - accuracy: 0.6587\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.7769 - accuracy: 0.6593\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.7574 - accuracy: 0.6687\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.7400 - accuracy: 0.6767\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.7403 - accuracy: 0.6880\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.7787 - accuracy: 0.6595\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.7623 - accuracy: 0.6595\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.7563 - accuracy: 0.6595\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.7557 - accuracy: 0.6609\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.7487 - accuracy: 0.6602\n",
      "7.884457349777222\n",
      "vali_acc= 0.6754666666666667\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.6858 - accuracy: 0.7207\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.6559 - accuracy: 0.7267\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.6354 - accuracy: 0.7380\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.6213 - accuracy: 0.7380\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.6038 - accuracy: 0.7473\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.7361 - accuracy: 0.6800\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.7107 - accuracy: 0.6880\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.7103 - accuracy: 0.7013\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.6919 - accuracy: 0.7127\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.6763 - accuracy: 0.7053\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.7631 - accuracy: 0.6647\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.7366 - accuracy: 0.6713\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.7257 - accuracy: 0.6720\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.7203 - accuracy: 0.6700\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.6976 - accuracy: 0.6813\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.7649 - accuracy: 0.6687\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.7227 - accuracy: 0.6827\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.7088 - accuracy: 0.6907\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.7012 - accuracy: 0.6973\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.6722 - accuracy: 0.7073\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.7378 - accuracy: 0.6682\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.7149 - accuracy: 0.6809\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.7071 - accuracy: 0.6802\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.7006 - accuracy: 0.6836\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.6873 - accuracy: 0.6862\n",
      "7.8632073402404785\n",
      "vali_acc= 0.6957333333333333\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.6458 - accuracy: 0.7273\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.6104 - accuracy: 0.7467\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5963 - accuracy: 0.7480\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5878 - accuracy: 0.7580\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5678 - accuracy: 0.7607\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.7001 - accuracy: 0.7080\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.6815 - accuracy: 0.7060\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.6734 - accuracy: 0.7167\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.6581 - accuracy: 0.7187\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.6470 - accuracy: 0.7227\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.7294 - accuracy: 0.6680\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.6909 - accuracy: 0.6893\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.6782 - accuracy: 0.6980\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.6739 - accuracy: 0.6880\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.6452 - accuracy: 0.7207\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.7182 - accuracy: 0.6900\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.6591 - accuracy: 0.7200\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.6477 - accuracy: 0.7113\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.6305 - accuracy: 0.7280\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.6110 - accuracy: 0.7287\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.7054 - accuracy: 0.6796\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.6734 - accuracy: 0.6936\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.6696 - accuracy: 0.6976\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.6653 - accuracy: 0.7049\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.6469 - accuracy: 0.7130\n",
      "8.115958452224731\n",
      "vali_acc= 0.7101333333333333\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.6169 - accuracy: 0.7400\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5845 - accuracy: 0.7487\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5740 - accuracy: 0.7720\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5618 - accuracy: 0.7693\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5542 - accuracy: 0.7740\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.6721 - accuracy: 0.7120\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.6503 - accuracy: 0.7247\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.6359 - accuracy: 0.7300\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.6268 - accuracy: 0.7313\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.6060 - accuracy: 0.7367\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.6916 - accuracy: 0.6793\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.6464 - accuracy: 0.7087\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.6324 - accuracy: 0.7227\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.6281 - accuracy: 0.7260\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.6184 - accuracy: 0.7320\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.6691 - accuracy: 0.7207\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.6250 - accuracy: 0.7213\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.6123 - accuracy: 0.7293\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5965 - accuracy: 0.7393\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5889 - accuracy: 0.7413\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.6818 - accuracy: 0.6876\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.6465 - accuracy: 0.7130\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.6392 - accuracy: 0.7230\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.6329 - accuracy: 0.7210\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.6183 - accuracy: 0.7283\n",
      "7.9684882164001465\n",
      "vali_acc= 0.7202666666666667\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.5846 - accuracy: 0.7520\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5654 - accuracy: 0.7740\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5460 - accuracy: 0.7760\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5437 - accuracy: 0.7760\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5199 - accuracy: 0.7867\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.6523 - accuracy: 0.7300\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.6249 - accuracy: 0.7367\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.6103 - accuracy: 0.7500\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5835 - accuracy: 0.7480\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5833 - accuracy: 0.7507\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.6620 - accuracy: 0.7020\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.6229 - accuracy: 0.7233\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.6130 - accuracy: 0.7373\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.6029 - accuracy: 0.7393\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5885 - accuracy: 0.7460\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.6444 - accuracy: 0.7273\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.6072 - accuracy: 0.7373\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5822 - accuracy: 0.7467\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5648 - accuracy: 0.7567\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5549 - accuracy: 0.7487\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.6625 - accuracy: 0.7043\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.6193 - accuracy: 0.7210\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.6222 - accuracy: 0.7230\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.6264 - accuracy: 0.7270\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.6044 - accuracy: 0.7310\n",
      "7.958450555801392\n",
      "vali_acc= 0.7285333333333334\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.5666 - accuracy: 0.7627\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5463 - accuracy: 0.7847\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5308 - accuracy: 0.7940\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5140 - accuracy: 0.7953\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5081 - accuracy: 0.7980\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.6335 - accuracy: 0.7347\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5965 - accuracy: 0.7507\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5805 - accuracy: 0.7547\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5644 - accuracy: 0.7627\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5496 - accuracy: 0.7700\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.6535 - accuracy: 0.7120\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5964 - accuracy: 0.7533\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5805 - accuracy: 0.7487\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5760 - accuracy: 0.7407\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.5616 - accuracy: 0.7520\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.6090 - accuracy: 0.7360\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5751 - accuracy: 0.7607\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5564 - accuracy: 0.7673\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5428 - accuracy: 0.7647\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5213 - accuracy: 0.7800\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.6492 - accuracy: 0.7069\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.6066 - accuracy: 0.7323\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.6036 - accuracy: 0.7417\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.5924 - accuracy: 0.7403\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5824 - accuracy: 0.7450\n",
      "8.145269632339478\n",
      "vali_acc= 0.7394666666666667\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.5518 - accuracy: 0.7767\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5183 - accuracy: 0.8000\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5078 - accuracy: 0.7953\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5021 - accuracy: 0.7973\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4791 - accuracy: 0.8160\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.6169 - accuracy: 0.7373\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5710 - accuracy: 0.7573\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5525 - accuracy: 0.7720\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5451 - accuracy: 0.7640\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5143 - accuracy: 0.7793\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.6206 - accuracy: 0.7220\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5610 - accuracy: 0.7580\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5541 - accuracy: 0.7640\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5252 - accuracy: 0.7767\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.5175 - accuracy: 0.7867\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.5923 - accuracy: 0.7473\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5617 - accuracy: 0.7660\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5292 - accuracy: 0.7813\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5212 - accuracy: 0.7787\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5092 - accuracy: 0.7853\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.6261 - accuracy: 0.7216\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5899 - accuracy: 0.7323\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5786 - accuracy: 0.7336\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5851 - accuracy: 0.7410\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5802 - accuracy: 0.7356\n",
      "7.903340101242065\n",
      "vali_acc= 0.7448\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.5389 - accuracy: 0.7867\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4978 - accuracy: 0.8040\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4895 - accuracy: 0.8067\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4748 - accuracy: 0.8207\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4465 - accuracy: 0.8247\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.6017 - accuracy: 0.7480\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5462 - accuracy: 0.7680\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5283 - accuracy: 0.7760\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5097 - accuracy: 0.7813\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4978 - accuracy: 0.7893\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.6002 - accuracy: 0.7433\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5505 - accuracy: 0.7620\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5232 - accuracy: 0.7840\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.4979 - accuracy: 0.7927\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4813 - accuracy: 0.8073\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.5746 - accuracy: 0.7553\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5313 - accuracy: 0.7813\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5054 - accuracy: 0.7980\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.4812 - accuracy: 0.8027\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4674 - accuracy: 0.8060\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.6078 - accuracy: 0.7316\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5750 - accuracy: 0.7470\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5698 - accuracy: 0.7383\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5536 - accuracy: 0.7503\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5729 - accuracy: 0.7450\n",
      "7.888680458068848\n",
      "vali_acc= 0.7528\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.5264 - accuracy: 0.7880\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4841 - accuracy: 0.8160\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4649 - accuracy: 0.8227\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4392 - accuracy: 0.8293\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4207 - accuracy: 0.8387\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.5819 - accuracy: 0.7593\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5279 - accuracy: 0.7800\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5011 - accuracy: 0.7807\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4849 - accuracy: 0.7887\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4731 - accuracy: 0.7947\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.5825 - accuracy: 0.7427\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5116 - accuracy: 0.7813\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4917 - accuracy: 0.7947\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4725 - accuracy: 0.8067\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4636 - accuracy: 0.8093\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.5492 - accuracy: 0.7713\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4941 - accuracy: 0.8087\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4786 - accuracy: 0.8007\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4646 - accuracy: 0.8120\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4414 - accuracy: 0.8247\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.5884 - accuracy: 0.7370\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5681 - accuracy: 0.7477\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5549 - accuracy: 0.7477\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5296 - accuracy: 0.7684\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5184 - accuracy: 0.7630\n",
      "7.913909435272217\n",
      "vali_acc= 0.7642666666666666\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.5102 - accuracy: 0.7933\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4685 - accuracy: 0.8233\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4356 - accuracy: 0.8333\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.4164 - accuracy: 0.8387\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.4060 - accuracy: 0.8480\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.5626 - accuracy: 0.7760\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4893 - accuracy: 0.7887\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4771 - accuracy: 0.7987\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.4699 - accuracy: 0.8033\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.4499 - accuracy: 0.8107\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.5552 - accuracy: 0.7553\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4937 - accuracy: 0.7893\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4641 - accuracy: 0.8093\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4517 - accuracy: 0.8107\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.4528 - accuracy: 0.8160\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.5231 - accuracy: 0.7807\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4782 - accuracy: 0.8100\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4519 - accuracy: 0.8147\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4323 - accuracy: 0.8287\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.4238 - accuracy: 0.8267\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.5595 - accuracy: 0.7617\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5287 - accuracy: 0.7744\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4936 - accuracy: 0.7850\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4638 - accuracy: 0.8077\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4633 - accuracy: 0.8051\n",
      "8.083207368850708\n",
      "vali_acc= 0.7664\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.4997 - accuracy: 0.8040\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4483 - accuracy: 0.8267\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4125 - accuracy: 0.8413\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.3991 - accuracy: 0.8453\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.3828 - accuracy: 0.8500\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.5329 - accuracy: 0.7767\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4724 - accuracy: 0.7953\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4551 - accuracy: 0.8073\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4438 - accuracy: 0.8140\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4195 - accuracy: 0.8153\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.5381 - accuracy: 0.7573\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4705 - accuracy: 0.8087\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4504 - accuracy: 0.8067\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4156 - accuracy: 0.8267\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4249 - accuracy: 0.8260\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.5186 - accuracy: 0.7960\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4696 - accuracy: 0.8113\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.4344 - accuracy: 0.8300\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.4080 - accuracy: 0.8387\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.4016 - accuracy: 0.8467\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.5369 - accuracy: 0.7644\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4950 - accuracy: 0.7917\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4685 - accuracy: 0.8077\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4472 - accuracy: 0.8097\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4322 - accuracy: 0.8151\n",
      "7.856548547744751\n",
      "vali_acc= 0.7752\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.4837 - accuracy: 0.8073\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4267 - accuracy: 0.8307\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.3898 - accuracy: 0.8453\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3762 - accuracy: 0.8573\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3553 - accuracy: 0.8660\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.5279 - accuracy: 0.7787\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4656 - accuracy: 0.7940\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4436 - accuracy: 0.8093\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4313 - accuracy: 0.8160\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4103 - accuracy: 0.8267\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.5268 - accuracy: 0.7807\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4606 - accuracy: 0.8153\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4260 - accuracy: 0.8267\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3926 - accuracy: 0.8433\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3726 - accuracy: 0.8547\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.5064 - accuracy: 0.7913\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4473 - accuracy: 0.8220\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4134 - accuracy: 0.8367\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3968 - accuracy: 0.8467\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.3810 - accuracy: 0.8480\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.5241 - accuracy: 0.7777\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4572 - accuracy: 0.8091\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4338 - accuracy: 0.8131\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.4167 - accuracy: 0.8284\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4027 - accuracy: 0.8264\n",
      "7.87654185295105\n",
      "vali_acc= 0.7773333333333333\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.4715 - accuracy: 0.8147\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4000 - accuracy: 0.8400\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3760 - accuracy: 0.8513\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3489 - accuracy: 0.8607\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3567 - accuracy: 0.8573\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.5152 - accuracy: 0.7867\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4447 - accuracy: 0.8027\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4309 - accuracy: 0.8153\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4116 - accuracy: 0.8213\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.3912 - accuracy: 0.8347\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.5092 - accuracy: 0.7833\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4387 - accuracy: 0.8153\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4018 - accuracy: 0.8327\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.3708 - accuracy: 0.8520\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.3539 - accuracy: 0.8580\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.4913 - accuracy: 0.8053\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4347 - accuracy: 0.8293\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3980 - accuracy: 0.8447\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3794 - accuracy: 0.8580\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3483 - accuracy: 0.8687\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.5054 - accuracy: 0.7830\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4362 - accuracy: 0.8231\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.4074 - accuracy: 0.8304\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3973 - accuracy: 0.8338\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3672 - accuracy: 0.8431\n",
      "7.819127321243286\n",
      "vali_acc= 0.7733333333333333\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.4531 - accuracy: 0.8247\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3785 - accuracy: 0.8567\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3508 - accuracy: 0.8667\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.3482 - accuracy: 0.8740\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3347 - accuracy: 0.8693\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.5031 - accuracy: 0.7927\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4160 - accuracy: 0.8293\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3978 - accuracy: 0.8313\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3887 - accuracy: 0.8420\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3712 - accuracy: 0.8447\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.4872 - accuracy: 0.7913\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4189 - accuracy: 0.8353\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.3754 - accuracy: 0.8480\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3654 - accuracy: 0.8507\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3496 - accuracy: 0.8640\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.4842 - accuracy: 0.8020\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4198 - accuracy: 0.8260\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3739 - accuracy: 0.8533\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3595 - accuracy: 0.8533\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3626 - accuracy: 0.8620\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.4842 - accuracy: 0.7977\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4184 - accuracy: 0.8364\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4027 - accuracy: 0.8284\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3708 - accuracy: 0.8438\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3564 - accuracy: 0.8618\n",
      "7.846414566040039\n",
      "vali_acc= 0.772\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.4562 - accuracy: 0.8300\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3738 - accuracy: 0.8567\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3481 - accuracy: 0.8627\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3193 - accuracy: 0.8780\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3058 - accuracy: 0.8813\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.4813 - accuracy: 0.8000\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3944 - accuracy: 0.8313\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3871 - accuracy: 0.8340\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3836 - accuracy: 0.8407\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3667 - accuracy: 0.8400\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.4851 - accuracy: 0.8007\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4091 - accuracy: 0.8307\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3553 - accuracy: 0.8580\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3299 - accuracy: 0.8807\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3225 - accuracy: 0.8760\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.4614 - accuracy: 0.8180\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4209 - accuracy: 0.8273\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3766 - accuracy: 0.8480\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3691 - accuracy: 0.8513\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3814 - accuracy: 0.8440\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.4714 - accuracy: 0.7957\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3817 - accuracy: 0.8445\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3651 - accuracy: 0.8511\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3366 - accuracy: 0.8685\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3390 - accuracy: 0.8632\n",
      "8.247912168502808\n",
      "vali_acc= 0.7805333333333333\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.4301 - accuracy: 0.8347\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3476 - accuracy: 0.8667\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.3238 - accuracy: 0.8800\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.2975 - accuracy: 0.8880\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.2807 - accuracy: 0.8913\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.4632 - accuracy: 0.8080\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3863 - accuracy: 0.8347\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3640 - accuracy: 0.8507\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.3431 - accuracy: 0.8587\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.3350 - accuracy: 0.8640\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.4743 - accuracy: 0.8067\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3788 - accuracy: 0.8447\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3406 - accuracy: 0.8707\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.3616 - accuracy: 0.8587\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3271 - accuracy: 0.8807\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.4425 - accuracy: 0.8207\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3727 - accuracy: 0.8487\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3416 - accuracy: 0.8680\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3447 - accuracy: 0.8747\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.3585 - accuracy: 0.8573\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.4711 - accuracy: 0.7911\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3883 - accuracy: 0.8378\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3561 - accuracy: 0.8598\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.3217 - accuracy: 0.8765\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.2971 - accuracy: 0.8932\n",
      "7.777215957641602\n",
      "vali_acc= 0.7789333333333334\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.4226 - accuracy: 0.8393\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3280 - accuracy: 0.8800\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.2912 - accuracy: 0.8913\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.2704 - accuracy: 0.9033\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.2737 - accuracy: 0.8920\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.4421 - accuracy: 0.8207\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3562 - accuracy: 0.8420\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3166 - accuracy: 0.8653\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.3000 - accuracy: 0.8747\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.3315 - accuracy: 0.8587\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.4427 - accuracy: 0.8253\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3584 - accuracy: 0.8580\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3173 - accuracy: 0.8860\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.2893 - accuracy: 0.8767\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.2733 - accuracy: 0.8893\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.4305 - accuracy: 0.8287\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3537 - accuracy: 0.8593\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3233 - accuracy: 0.8747\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.3200 - accuracy: 0.8833\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3266 - accuracy: 0.8680\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.4400 - accuracy: 0.8158\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3454 - accuracy: 0.8605\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.3157 - accuracy: 0.8718\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3086 - accuracy: 0.8758\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3128 - accuracy: 0.8818\n",
      "7.789516448974609\n",
      "vali_acc= 0.7797333333333333\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.4216 - accuracy: 0.8313\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3223 - accuracy: 0.8793\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.2921 - accuracy: 0.8860\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.2515 - accuracy: 0.9060\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.2561 - accuracy: 0.9033\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.4308 - accuracy: 0.8180\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3179 - accuracy: 0.8653\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.2945 - accuracy: 0.8787\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.2643 - accuracy: 0.8920\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.2704 - accuracy: 0.8940\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.4197 - accuracy: 0.8300\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3334 - accuracy: 0.8707\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3053 - accuracy: 0.8833\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.2831 - accuracy: 0.8940\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.2257 - accuracy: 0.9247\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.4080 - accuracy: 0.8307\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3237 - accuracy: 0.8707\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.2921 - accuracy: 0.8947\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.2932 - accuracy: 0.8847\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3224 - accuracy: 0.8680\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.4458 - accuracy: 0.8138\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3345 - accuracy: 0.8652\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3026 - accuracy: 0.8872\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.2962 - accuracy: 0.8892\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.2557 - accuracy: 0.9025\n",
      "7.803986072540283\n",
      "vali_acc= 0.7741333333333333\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.4137 - accuracy: 0.8413\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.2864 - accuracy: 0.9000\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.2652 - accuracy: 0.9020\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.2370 - accuracy: 0.9133\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.2323 - accuracy: 0.9227\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.4097 - accuracy: 0.8360\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3288 - accuracy: 0.8593\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.2976 - accuracy: 0.8880\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.2600 - accuracy: 0.8947\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.2890 - accuracy: 0.8847\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.4300 - accuracy: 0.8240\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3136 - accuracy: 0.8827\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.2991 - accuracy: 0.8833\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.2674 - accuracy: 0.8980\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.2373 - accuracy: 0.9180\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.4197 - accuracy: 0.8307\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3415 - accuracy: 0.8647\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.3445 - accuracy: 0.8500\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.2970 - accuracy: 0.8840\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.2605 - accuracy: 0.9013\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 5ms/step - loss: 0.4229 - accuracy: 0.8271\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3097 - accuracy: 0.8632\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.2681 - accuracy: 0.9039\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.2605 - accuracy: 0.8959\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.2609 - accuracy: 0.8972\n",
      "7.8359901905059814\n",
      "vali_acc= 0.7794666666666666\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The avg time for per communication7.93767260313034\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "vali_accuracy=[]\n",
    "time_arr=[]\n",
    "for m in range (20):\n",
    "    y_pred=global_model(vali_x)\n",
    "    y_pred=tf.argmax(y_pred, axis=1)\n",
    "    acc=acc_calculator(vali_y,y_pred)\n",
    "    vali_accuracy.append(acc)\n",
    "    start=time.time()\n",
    "    w=Fedavg()\n",
    "    end=time.time()\n",
    "    global_model.set_weights(w)\n",
    "    time_count=end-start\n",
    "    time_arr.append(time_count)\n",
    "    print(end-start)\n",
    "    print('vali_acc=',acc)\n",
    "plt.plot(vali_accuracy,'b--',label='FedAvg')\n",
    "plt.xlabel('# of rounds')\n",
    "plt.ylabel('validation accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "print('The avg time for per communication'+str(np.mean(time_arr)))\n",
    "global_model.save_weights('Fed_Avg.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8927e6531f7cb1d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T22:20:40.550266Z",
     "start_time": "2024-09-09T22:20:40.547177Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average time for 1 communication round 7.93767260313034\n"
     ]
    }
   ],
   "source": [
    "avg_time=sum(time_arr)/len(time_arr)\n",
    "print('average time for 1 communication round', avg_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659f3ec3256f1871",
   "metadata": {},
   "source": [
    "# global and local fairness evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45ee3fb554b518a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T22:20:40.659046Z",
     "start_time": "2024-09-09T22:20:40.654226Z"
    }
   },
   "outputs": [],
   "source": [
    "def EO_compuatation(y_pred, y_ture,s_sensitive):\n",
    "    EO_list=[]\n",
    "    y_pred=np.reshape(y_pred,(-1,))\n",
    "    y_ture=np.reshape(y_ture,(-1,))\n",
    "    s_sensitive=np.reshape(s_sensitive,(-1,))\n",
    "    for i in range (4):\n",
    "        tpr_sensitive=np.sum(np.where((y_pred==i)&(y_ture==i)&(s_sensitive==1),1,0))/np.sum(np.where((y_ture==i)&(s_sensitive==1),1,0))\n",
    "        tpr_nonsensitive=np.sum(np.where((y_pred==i)&(y_ture==i)&(s_sensitive==0),1,0))/np.sum(np.where((y_ture==i)&(s_sensitive==0),1,0))\n",
    "        EO=abs(tpr_sensitive-tpr_nonsensitive)\n",
    "        if EO<1 or EO==1:\n",
    "            EO_list.append(EO)\n",
    "        else:\n",
    "            EO_list.append(0.01)\n",
    "    print(EO_list)\n",
    "    EO=max(EO_list)\n",
    "    return EO\n",
    "def SP_compuatation(y_pred, y_ture,s_sensitive):\n",
    "    SP_list=[]\n",
    "    y_pred=np.reshape(y_pred,(-1,))\n",
    "    y_ture=np.reshape(y_ture,(-1,))\n",
    "    s_sensitive=np.reshape(s_sensitive,(-1,))\n",
    "    for i in range (4):\n",
    "        tpr_sensitive=np.sum(np.where((y_pred==i)&(s_sensitive==1),1,0))/np.sum(np.where((s_sensitive==1),1,0))\n",
    "        tpr_nonsensitive=np.sum(np.where((y_pred==i)&(s_sensitive==0),1,0))/np.sum(np.where((s_sensitive==0),1,0))\n",
    "        SP=abs(tpr_sensitive-tpr_nonsensitive)\n",
    "        if SP<1 or SP==1:\n",
    "            SP_list.append(SP)\n",
    "        else:\n",
    "            SP_list.append(0)\n",
    "    SP=max(SP_list)\n",
    "    print(SP_list)\n",
    "    return SP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64aedc86b88ab54",
   "metadata": {},
   "source": [
    "evaluation of group fairness on validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b39fdb995c7b2a34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T22:20:40.816684Z",
     "start_time": "2024-09-09T22:20:40.703893Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3750, 28, 28, 3)\n",
      "[0.0752812574802521, 0.1215764725292856, 0.03433467262463541, 0.15664160401002503]\n",
      "vali_EO= 0.15664160401002503\n",
      "[0.06708742216800787, 0.0008483004405927946, 0.0655120070640498, 0.0024237155445508488]\n",
      "vali_SP= 0.06708742216800787\n"
     ]
    }
   ],
   "source": [
    "y_pred=global_model(vali_x)\n",
    "print(np.shape(vali_x))\n",
    "y_pred=tf.argmax(y_pred, axis=1)\n",
    "vali_EO=EO_compuatation(y_pred, vali_y,vali_s)\n",
    "print('vali_EO=',vali_EO)\n",
    "vali_SP=SP_compuatation(y_pred, vali_y,vali_s)\n",
    "print('vali_SP=',vali_SP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0308356a38577",
   "metadata": {},
   "source": [
    "# evaluation of local fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c4be224f90fb903",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T22:20:41.078107Z",
     "start_time": "2024-09-09T22:20:40.984702Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.21792035398230086, 0.1875, 0.0461087420042644, 0.4666666666666667]\n",
      "[0.10101010101010105, 0.25252525252525254, 0.0604552733928162, 0.4444444444444444]\n",
      "[0.07238095238095232, 0.2219973009446694, 0.01568055960361414, 0.2222222222222222]\n",
      "[0.01693548387096777, 0.21062740076824588, 0.042705108359133104, 0.08333333333333337]\n",
      "[0.1911081619110816, 0.22222222222222227, 0.02965288679574396, 0.30000000000000004]\n",
      "local_EO= 0.3287921468203158\n",
      "local_EO= [0.4666666666666667, 0.4444444444444444, 0.2222222222222222, 0.21062740076824588, 0.30000000000000004]\n",
      "[0.09805548248720178, 0.013010663483134705, 0.07470508488294447, 0.0103397341211226]\n",
      "[0.1258748292249537, 0.024658659447317474, 0.09406666722543988, 0.007149502552196398]\n",
      "[0.06477294575906686, 0.0056042555312649095, 0.060692591985985755, 0.009684609304345969]\n",
      "[0.05679591974599743, 0.01464230223603323, 0.0709146794568668, 0.000523542525163817]\n",
      "[0.019265902673974433, 0.03426894757238555, 0.051652549410396875, 0.0018823008359630175]\n",
      "local_SP= [0.09805548248720178, 0.1258748292249537, 0.06477294575906686, 0.0709146794568668, 0.051652549410396875]\n"
     ]
    }
   ],
   "source": [
    "vali_x_1=np.reshape(vali_x_1,(-1,28,28,3))\n",
    "vali_x_2=np.reshape(vali_x_2,(-1,28,28,3))\n",
    "vali_x_3=np.reshape(vali_x_3,(-1,28,28,3))\n",
    "vali_x_4=np.reshape(vali_x_4,(-1,28,28,3))\n",
    "vali_x_5=np.reshape(vali_x_5,(-1,28,28,3))\n",
    "\n",
    "local_EO=[]\n",
    "y_pred_1=global_model(vali_x_1)\n",
    "y_pred_1=tf.argmax(y_pred_1, axis=1)\n",
    "EO_1=EO_compuatation(y_pred_1, vali_y_1,vali_s_1)\n",
    "local_EO.append(EO_1)\n",
    "y_pred_2=global_model(vali_x_2)\n",
    "y_pred_2=tf.argmax(y_pred_2, axis=1)\n",
    "EO_2=EO_compuatation(y_pred_2, vali_y_2,vali_s_2)\n",
    "local_EO.append(EO_2)\n",
    "y_pred_3=global_model(vali_x_3)\n",
    "y_pred_3=tf.argmax(y_pred_3, axis=1)\n",
    "EO_3=EO_compuatation(y_pred_3, vali_y_3,vali_s_3)\n",
    "local_EO.append(EO_3)\n",
    "y_pred_4=global_model(vali_x_4)\n",
    "y_pred_4=tf.argmax(y_pred_4, axis=1)\n",
    "EO_4=EO_compuatation(y_pred_4, vali_y_4,vali_s_4)\n",
    "local_EO.append(EO_4)\n",
    "y_pred_5=global_model(vali_x_5)\n",
    "y_pred_5=tf.argmax(y_pred_5, axis=1)\n",
    "EO_5=EO_compuatation(y_pred_5, vali_y_5,vali_s_5)\n",
    "local_EO.append(EO_5)\n",
    "local_EO_avg=sum(local_EO)/len(local_EO)\n",
    "print('local_EO=',local_EO_avg)\n",
    "print('local_EO=',local_EO)\n",
    "local_SP=[]\n",
    "SP_1=SP_compuatation(y_pred_1, vali_y_1,vali_s_1)\n",
    "local_SP.append(SP_1)\n",
    "SP_2=SP_compuatation(y_pred_2, vali_y_2,vali_s_2)\n",
    "local_SP.append(SP_2)\n",
    "SP_3=SP_compuatation(y_pred_3, vali_y_3,vali_s_3)\n",
    "local_SP.append(SP_3)\n",
    "SP_4=SP_compuatation(y_pred_4, vali_y_4,vali_s_4)\n",
    "local_SP.append(SP_4)\n",
    "SP_5=SP_compuatation(y_pred_5, vali_y_5,vali_s_5)\n",
    "local_SP.append(SP_5)\n",
    "print('local_SP=',local_SP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd10abe04086382",
   "metadata": {},
   "source": [
    "Evaluation fairness on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f8cf4138cea28f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T22:20:41.446841Z",
     "start_time": "2024-09-09T22:20:41.444840Z"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c75285f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_acc= 0.7818666666666667\n"
     ]
    }
   ],
   "source": [
    "y_pred=global_model(vali_x)\n",
    "y_pred=tf.argmax(y_pred, axis=1)\n",
    "acc_count=np.where(y_pred==vali_y,1,0)\n",
    "acc=sum(acc_count)/len(vali_y)\n",
    "print('global_acc=',acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95b8918",
   "metadata": {},
   "source": [
    "# achieving fairness via LP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3ae83a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.03013333 0.02133333 0.12506667 0.004     ]\n",
      "  [0.00426667 0.00213333 0.0128     0.00026667]]\n",
      "\n",
      " [[0.02053333 0.0168     0.0992     0.0024    ]\n",
      "  [0.0144     0.0088     0.0376     0.00026667]]\n",
      "\n",
      " [[0.01333333 0.01013333 0.07786667 0.0024    ]\n",
      "  [0.0224     0.0104     0.06266667 0.0008    ]]\n",
      "\n",
      " [[0.01066667 0.00586667 0.04266667 0.00106667]\n",
      "  [0.03306667 0.01893333 0.08613333 0.0016    ]]\n",
      "\n",
      " [[0.00586667 0.0016     0.01386667 0.00026667]\n",
      "  [0.03653333 0.0216     0.1176     0.00266667]]]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def p_computation(label,sensitive_attribute,y,a):\n",
    "    label=np.reshape(label,(-1,))\n",
    "    sensitive_attribute=np.reshape(sensitive_attribute,(-1,))\n",
    "    N=len(label)\n",
    "    mask=(label==y)&(sensitive_attribute==a)\n",
    "    p_y_a=np.sum(mask)/N\n",
    "    return p_y_a\n",
    "S=np.zeros((5,2,4))\n",
    "p=[len(vali_x_1),len(vali_x_2),len(vali_x_3),len(vali_x_4),len(vali_x_5)]  \n",
    "for c,(feature,labels,sensitive) in enumerate(([vali_x_1,vali_y_1,vali_s_1],[vali_x_2,vali_y_2,vali_s_2],[vali_x_3,vali_y_3,vali_s_3],[vali_x_4,vali_y_4,vali_s_4],[vali_x_5,vali_y_5,vali_s_5])):\n",
    "    for a in range(2):\n",
    "        for y in range(4):\n",
    "            S[c][a][y]=p[c]*p_computation(labels,sensitive,y,a)/(len(vali_x_1)+len(vali_x_2)+len(vali_x_3)+len(vali_x_4)+len(vali_x_5))\n",
    "print(S)\n",
    "print(np.sum(S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce10c90e99e21ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T22:20:42.089082Z",
     "start_time": "2024-09-09T22:20:41.845343Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.40707965 0.5625     0.92110874 0.46666667]\n",
      "  [0.625      0.75       0.875      0.        ]]\n",
      "\n",
      " [[0.45454545 0.55555556 0.9327957  0.55555556]\n",
      "  [0.55555556 0.3030303  0.87234043 1.        ]]\n",
      "\n",
      " [[0.62       0.65789474 0.91780822 0.77777778]\n",
      "  [0.54761905 0.43589744 0.90212766 1.        ]]\n",
      "\n",
      " [[0.525      0.59090909 0.93125    0.75      ]\n",
      "  [0.50806452 0.38028169 0.88854489 0.83333333]]\n",
      "\n",
      " [[0.36363636 0.33333333 0.92307692 1.        ]\n",
      "  [0.55474453 0.55555556 0.89342404 0.7       ]]]\n"
     ]
    }
   ],
   "source": [
    "TP=np.zeros((5,2,4))\n",
    "def compute_TP (y_pred,label, sensitive_attribute, y, a):\n",
    "    y_pred=np.reshape(y_pred,(-1,))\n",
    "    label=np.reshape(label,(-1,))\n",
    "    N=len(y_pred)\n",
    "    sensitive_attribute=np.reshape(sensitive_attribute,(-1,))# Total number of samples\n",
    "    # Create a boolean mask for where both conditions are met\n",
    "    count_1= (y_pred == y) & (label == y) & (sensitive_attribute == a)\n",
    "    count_2 = (label== y) & (sensitive_attribute == a)\n",
    "    \n",
    "    # Compute the probability\n",
    "    TP_y_ac = np.sum(count_1) / np.sum(count_2)\n",
    "    if TP_y_ac<1 or TP_y_ac==1:\n",
    "        TP_y_ac=TP_y_ac\n",
    "    else:\n",
    "        TP_y_ac=0\n",
    "    return TP_y_ac\n",
    "for c,(feature,labels,sensitive) in enumerate (([vali_x_1,vali_y_1,vali_s_1],[vali_x_2,vali_y_2,vali_s_2],[vali_x_3,vali_y_3,vali_s_3],[vali_x_4,vali_y_4,vali_s_4],[vali_x_5,vali_y_5,vali_s_5])):\n",
    "    y_pred=global_model(feature)\n",
    "    y_pred=tf.argmax(y_pred, axis=1)\n",
    "    for a in range(2):\n",
    "        for y in range(4):\n",
    "            TP[c][a][y]=compute_TP(y_pred,labels,sensitive,y,a)\n",
    "print(TP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "414d6201e39f3c89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T22:20:42.247506Z",
     "start_time": "2024-09-09T22:20:42.245860Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.08053333 0.05573333 0.35866667 0.01013333]\n",
      " [0.11066667 0.06186667 0.3168     0.0056    ]]\n"
     ]
    }
   ],
   "source": [
    "def compute_alpha(label, sensitive_attribute, y, a):\n",
    "    label=np.reshape(label,(-1,))\n",
    "    N=len(label)\n",
    "    sensitive_attribute=np.reshape(sensitive_attribute,(-1,))# Total number of samples\n",
    "    # Create a boolean mask for where both conditions are met\n",
    "    mask = (label == y) & (sensitive_attribute == a)\n",
    "    # print(mask)\n",
    "    # print(\"Number of matching samples:\", np.sum(mask))\n",
    "    \n",
    "    # Compute the probability\n",
    "    alpha_y_ac = np.sum(mask) / N\n",
    "    return alpha_y_ac\n",
    "alpha_a_y=np.zeros((2,4))\n",
    "for a in range(2):\n",
    "    for y in range(4):\n",
    "        alpha_a_y[a][y]=compute_alpha(vali_y,vali_s,y,a)\n",
    "print(alpha_a_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff8ff40",
   "metadata": {},
   "source": [
    "# LP for fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b9918b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 2, 4)\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import linprog\n",
    "import numpy as np\n",
    "def LP_EO(e_0,e_c):\n",
    "    C=[]\n",
    "    for c in range(5):\n",
    "        for a in range(2):\n",
    "            for y in range(4):\n",
    "                C.append(-S[c][a][y])\n",
    "    ## global fairness constraints\n",
    "    A_1=[]\n",
    "    A_2=[]\n",
    "    A_3=[]\n",
    "    A_4=[]\n",
    "\n",
    "    for c in range(5):\n",
    "        for a in range(2):\n",
    "            for y in range(4):\n",
    "                if a==0 and y==0:\n",
    "                    A_1.append(-S[c][a][y]/alpha_a_y[a][y])\n",
    "                elif a==1 and y==0:\n",
    "                    A_1.append(S[c][a][y]/alpha_a_y[a][y])\n",
    "                else:\n",
    "                    A_1.append(0)\n",
    "    for c in range(5):\n",
    "        for a in range(2):\n",
    "            for y in range(4):\n",
    "                if a==0 and y==1:\n",
    "                    A_2.append(-S[c][a][y]/alpha_a_y[a][y])\n",
    "                elif a==1 and y==1:\n",
    "                    A_2.append(S[c][a][y]/alpha_a_y[a][y])\n",
    "                else:\n",
    "                    A_2.append(0)\n",
    "    for c in range(5):\n",
    "        for a in range(2):\n",
    "            for y in range(4):\n",
    "                if a==0 and y==2:\n",
    "                    A_3.append(-S[c][a][y]/alpha_a_y[a][y])\n",
    "                elif a==1 and y==2:\n",
    "                    A_3.append(S[c][a][y]/alpha_a_y[a][y])\n",
    "                else:\n",
    "                    A_3.append(0)\n",
    "    for c in range(5):\n",
    "        for a in range(2):\n",
    "            for y in range(4):\n",
    "                if a==0 and y==3:\n",
    "                    A_4.append(-S[c][a][y]/alpha_a_y[a][y])\n",
    "                elif a==1 and y==3:\n",
    "                    A_4.append(S[c][a][y]/alpha_a_y[a][y])\n",
    "\n",
    "                else:\n",
    "                    A_4.append(0)\n",
    "    A_1=np.array(A_1)\n",
    "    A_2=np.array(A_2)\n",
    "    A_3=np.array(A_3)\n",
    "    A_4=np.array(A_4)\n",
    "    ## define local fairness constraints\n",
    "    basis_vector=np.eye(4)\n",
    "    zero=np.zeros((4,4))\n",
    "    row_1=np.hstack((basis_vector, -basis_vector, zero, zero, zero, zero, zero, zero, zero, zero))\n",
    "    row_2=np.hstack((zero,zero, basis_vector, -basis_vector, zero, zero, zero, zero, zero, zero))\n",
    "    row_3=np.hstack((zero,zero, zero, zero, basis_vector, -basis_vector, zero, zero, zero, zero))\n",
    "    row_4=np.hstack((zero,zero, zero, zero, zero, zero, basis_vector, -basis_vector, zero, zero))\n",
    "    row_5=np.hstack((zero,zero, zero, zero, zero, zero, zero, zero, basis_vector, -basis_vector))\n",
    "    A_l=np.vstack((row_1,row_2,row_3,row_4,row_5))\n",
    "    def K_ac_compute(a,c):\n",
    "        K_ac = np.zeros((5, 4))  # 5 rows, 4 columns\n",
    "        l_ac = np.zeros((5, 1))  # 5 rows, 1 column\n",
    "\n",
    "        # Modify this line to assign only 4 elements (to match the number of columns in K_ac)\n",
    "        K_ac[0:] = [-1, -1, -1, -1]  # 4 elements, matching the number of columns\n",
    "\n",
    "        # Similarly adjust the rest of the assignments to match the column count of 4\n",
    "        K_ac[1:] = [(1 - (TP[c, a, 1] + TP[c, a, 2] + TP[c, a, 3])), TP[c, a, 0], TP[c, a, 0], TP[c, a, 0]]\n",
    "        K_ac[2:] = [TP[c, a, 1], (1 - (TP[c, a, 0] + TP[c, a, 2] + TP[c, a, 3])), TP[c, a, 1], TP[c, a, 1]]\n",
    "        K_ac[3:] = [TP[c, a, 2], TP[c, a, 2], (1 - (TP[c, a, 0] + TP[c, a, 1] + TP[c, a, 3])), TP[c, a, 2]]\n",
    "        K_ac[4:] = [TP[c, a, 3], TP[c, a, 3], TP[c, a, 3], (1 - (TP[c, a, 0] + TP[c, a, 1] + TP[c, a, 2]))]\n",
    "\n",
    "        l_ac[0:] = [-1]\n",
    "        l_ac[1:] = [TP[c, a, 0]]\n",
    "        l_ac[2:] = [TP[c, a, 1]]\n",
    "        l_ac[3:] = [TP[c, a, 2]]\n",
    "        l_ac[4:] = [TP[c, a, 3]]\n",
    "        return K_ac,l_ac\n",
    "    k_01, l_01 = K_ac_compute(0, 0)\n",
    "    k_11, l_11 = K_ac_compute(1, 0)\n",
    "    k_02, l_02 = K_ac_compute(0, 1)\n",
    "    k_12, l_12 = K_ac_compute(1, 1)\n",
    "    k_03, l_03 = K_ac_compute(0, 2)\n",
    "    k_13, l_13 = K_ac_compute(1, 2)\n",
    "    k_04, l_04 = K_ac_compute(0, 3)\n",
    "    k_14, l_14 = K_ac_compute(1, 3)\n",
    "    k_05, l_05 = K_ac_compute(0, 4)\n",
    "    k_15, l_15 = K_ac_compute(1, 4)\n",
    "\n",
    "\n",
    "    M = np.zeros((50,40))  # 10 blocks of 6x5 matrices; resulting in a 60x25 matrix\n",
    "    l = np.zeros((50, 1))   # Vector to match the 60 rows\n",
    "\n",
    "    # Place each submatrix on the diagonal\n",
    "    M[0:5, 0:4] = k_01    # Place k_01 in the top-left\n",
    "    M[5:10, 4:8] = k_11  # Place k_11 in the next diagonal block\n",
    "    M[10:15, 8:12] = k_02  # Place k_02 in the next diagonal block\n",
    "    M[15:20, 12:16] = k_12  # Place k_12 in the next diagonal block\n",
    "    M[20:25, 16:20] = k_03  # Place k_03 in the next diagonal block\n",
    "    M[25:30, 20:24] = k_13    # Next row of blocks, place k_13\n",
    "    M[30:35, 24:28] = k_04  # Place k_14 in the next block\n",
    "    M[35:40, 28:32] = k_14 # Continue placing the remaining matrices\n",
    "    M[40:45, 32:36] = k_05 # Place k_05\n",
    "    M[45:50, 36:40] = k_15 # Place k_15 in the last block\n",
    "\n",
    "    # Construct vector l\n",
    "    l[0:5] = l_01\n",
    "    l[5:10] = l_11\n",
    "    l[10:15] = l_02\n",
    "    l[15:20] = l_12\n",
    "    l[20:25] = l_03\n",
    "    l[25:30] = l_13\n",
    "    l[30:35] = l_04\n",
    "    l[35:40] = l_14\n",
    "    l[40:45] = l_05\n",
    "    l[45:50] = l_15\n",
    "    A=np.vstack((A_1,A_2,A_3,A_4,A_l,-A_1,-A_2,-A_3,-A_4,-A_l,M))\n",
    "    b_global=e_0*np.ones((4,1))\n",
    "    b_local=e_c*np.ones((20,1))\n",
    "    # print(b_local)\n",
    "    b=np.vstack((b_global,b_local,b_global,b_local,l))\n",
    "    # print(b)\n",
    "    res = linprog(C, A_ub=A, b_ub=b)\n",
    "    x=res.x\n",
    "    x=np.reshape(x,(5,2,4))\n",
    "    # print(x)\n",
    "    # print(res.fun)\n",
    "    return x\n",
    "x=LP_EO(0.01,0.01)\n",
    "print(np.shape(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1db12be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.         0.01831691 0.02531019 0.99645022 0.02099808]\n",
      "[[1.        ]\n",
      " [0.01831691]\n",
      " [0.02531019]\n",
      " [0.99645022]\n",
      " [0.02099808]]\n",
      "[1.         0.01275849 0.01531019 0.98645022 0.01099808]\n",
      "[[1.        ]\n",
      " [0.01275849]\n",
      " [0.01531019]\n",
      " [0.98645022]\n",
      " [0.01099808]]\n",
      "[1.         0.0252941  0.03091502 0.99626028 0.03091502]\n",
      "[[1.        ]\n",
      " [0.0252941 ]\n",
      " [0.03091502]\n",
      " [0.99626028]\n",
      " [0.03091502]]\n",
      "[1.         0.02273056 0.02091502 0.98626028 0.04091502]\n",
      "[[1.        ]\n",
      " [0.02273056]\n",
      " [0.02091502]\n",
      " [0.98626028]\n",
      " [0.04091502]]\n",
      "[1.         0.04634274 0.04917523 0.99385646 0.05813605]\n",
      "[[1.        ]\n",
      " [0.04634274]\n",
      " [0.04917523]\n",
      " [0.99385646]\n",
      " [0.05813605]]\n",
      "[1.         0.0373126  0.03917523 0.98385646 0.06813605]\n",
      "[[1.        ]\n",
      " [0.0373126 ]\n",
      " [0.03917523]\n",
      " [0.98385646]\n",
      " [0.06813605]]\n",
      "[1.         0.03074846 0.03460865 0.99597342 0.04392636]\n",
      "[[1.        ]\n",
      " [0.03074846]\n",
      " [0.03460865]\n",
      " [0.99597342]\n",
      " [0.04392636]]\n",
      "[1.         0.03287769 0.02460865 0.99278756 0.05392636]\n",
      "[[1.        ]\n",
      " [0.03287769]\n",
      " [0.02460865]\n",
      " [0.99278756]\n",
      " [0.05392636]]\n",
      "[1.         0.01933644 0.01937933 0.99043756 0.04701795]\n",
      "[[1.        ]\n",
      " [0.01933644]\n",
      " [0.01937933]\n",
      " [0.99043756]\n",
      " [0.04701795]]\n",
      "[1.         0.02933644 0.02937933 0.99436397 0.03701795]\n",
      "[[1.        ]\n",
      " [0.02933644]\n",
      " [0.02937933]\n",
      " [0.99436397]\n",
      " [0.03701795]]\n",
      "24/24 [==============================] - 0s 2ms/step\n",
      "24/24 [==============================] - 0s 2ms/step\n",
      "24/24 [==============================] - 0s 2ms/step\n",
      "24/24 [==============================] - 0s 1ms/step\n",
      "24/24 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "vali_x_1=np.reshape(vali_x_1,(-1,28,28,3))\n",
    "vali_x_2=np.reshape(vali_x_2,(-1,28,28,3))\n",
    "vali_x_3=np.reshape(vali_x_3,(-1,28,28,3))\n",
    "vali_x_4=np.reshape(vali_x_4,(-1,28,28,3))\n",
    "vali_x_5=np.reshape(vali_x_5,(-1,28,28,3))\n",
    "beta=np.zeros((5,2,5))\n",
    "for c in range(5):\n",
    "        for a in range(2):\n",
    "            C=[0,0,0,0,0]\n",
    "            A=np.array([[1,1,1,1,1],[TP[c,a,0],1,0,0,0],[TP[c,a,1],0,1,0,0],[TP[c,a,2],0,0,1,0],[TP[c,a,3],0,0,0,1]])\n",
    "            b=np.array([1,x[c,a,0],x[c,a,1],x[c,a,2],x[c,a,3]])\n",
    "            b=np.reshape(b,(5,1))\n",
    "            res = linprog(C, A_eq=A, b_eq=b, bounds=(0,1))\n",
    "            beta_ac=res.x\n",
    "            beta[c,a,:]=beta_ac\n",
    "            e=np.matmul(A,beta_ac)\n",
    "            print(e)\n",
    "            print(b)\n",
    "beta=np.reshape(beta,(5,2,5))\n",
    "\n",
    "\n",
    "\n",
    "def compute_tilde_Y(c,a, Y_hat):\n",
    "    \"\"\"\n",
    "    Compute \\widetilde{Y}_{\\boldsymbol{\\beta}_{ac}}(x,a,c) based on given probabilities.\n",
    "\n",
    "    Parameters:\n",
    "    - beta_ac: Dictionary with keys 'beta_0' and 'beta_y' for probabilities.\n",
    "    - Y_hat: The predicted value \\hat{Y}(x,a,c).\n",
    "    - y_values: List or array of possible y values in \\mathcal{Y}.\n",
    "\n",
    "    Returns:\n",
    "    - tilde_Y: The computed value of \\widetilde{Y}.\n",
    "    \"\"\"\n",
    "    # Extract probabilities\n",
    "    beta_0 = beta[c, a, 0]  # Probability for Y_hat\n",
    "    beta_ac = beta[c, a, :] \n",
    "    # Probabilities for other y in \\mathcal{Y}\n",
    "    \n",
    "    # Normalize probabilities\n",
    "    \n",
    "    # Generate a random number\n",
    "    rand_val = random.random()\n",
    "    # Determine the output based on random value\n",
    "    if rand_val < beta_0:\n",
    "        return Y_hat\n",
    "    elif rand_val < beta_0 + beta_ac[1]:\n",
    "        return 0\n",
    "    elif rand_val < beta_0 + beta_ac[1] + beta_ac[2]:\n",
    "        return 1\n",
    "    elif rand_val < beta_0 + beta_ac[1] + beta_ac[2] + beta_ac[3]:\n",
    "        return 2\n",
    "    else: # Last value\n",
    "        return 3\n",
    "\n",
    "y_tilde_1=[]\n",
    "y_tilde_2=[]\n",
    "y_tilde_3=[]\n",
    "y_tilde_4=[]\n",
    "y_tilde_5=[]\n",
    "\n",
    "# Example usage\n",
    "for c, (feature, label, sensitive) in enumerate([(vali_x_1, vali_y_1, vali_s_1), (vali_x_2, vali_y_2, vali_s_2), (vali_x_3, vali_y_3, vali_s_3), (vali_x_4, vali_y_4, vali_s_4), (vali_x_5, vali_y_5, vali_s_5)]):\n",
    "    y_pred=global_model.predict(feature)\n",
    "    y_pred=tf.argmax(y_pred, axis=1)\n",
    "    for i in range (len(y_pred)):\n",
    "        a=sensitive[i]\n",
    "        y_hat=y_pred[i]\n",
    "        y_tilde=compute_tilde_Y(c,a,y_hat)\n",
    "        if c==0:\n",
    "            y_tilde_1.append(y_tilde)\n",
    "        elif c==1:\n",
    "            y_tilde_2.append(y_tilde)\n",
    "        elif c==2:\n",
    "            y_tilde_3.append(y_tilde)\n",
    "        elif c==3:\n",
    "            y_tilde_4.append(y_tilde)\n",
    "        elif c==4:\n",
    "            y_tilde_5.append(y_tilde)\n",
    "\n",
    "  \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d211d44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.010556131812016277, 0.0033204091734037287, 0.0015295457674640023, 0.09523809523809523]\n",
      "global_EO= 0.09523809523809523\n",
      "[0.017699115044247787, 0.025, 0.016568941009239557, 0.0]\n",
      "[0.00553150553150553, 0.031746031746031744, 0.014184397163120588, 0.0]\n",
      "[0.015714285714285712, 0.05330634278002699, 0.002491984844068762, 0.3333333333333333]\n",
      "[0.009677419354838714, 0.04225352112676056, 0.009404024767801777, 0.16666666666666666]\n",
      "[0.008958195089581955, 0.037037037037037035, 0.0022675736961451642, 0.0]\n",
      "local_EO= [0.025, 0.031746031746031744, 0.3333333333333333, 0.16666666666666666, 0.037037037037037035]\n",
      "local_EO_avg_post= 0.11875661375661375\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vali_tiled_y=np.concatenate((y_tilde_1,y_tilde_2,y_tilde_3,y_tilde_4,y_tilde_5),axis=0)\n",
    "vali_tiled_y=np.reshape(vali_tiled_y,(-1,))\n",
    "global_EO=EO_compuatation(vali_tiled_y,vali_y,vali_s)\n",
    "print('global_EO=',global_EO)\n",
    "local_EO_list_post=[]\n",
    "local_EO_1=EO_compuatation(y_tilde_1,vali_y_1,vali_s_1)\n",
    "local_EO_list_post.append(local_EO_1)\n",
    "local_EO_2=EO_compuatation(y_tilde_2,vali_y_2,vali_s_2)\n",
    "local_EO_list_post.append(local_EO_2)\n",
    "local_EO_3=EO_compuatation(y_tilde_3,vali_y_3,vali_s_3)\n",
    "local_EO_list_post.append(local_EO_3)\n",
    "local_EO_4=EO_compuatation(y_tilde_4,vali_y_4,vali_s_4)\n",
    "local_EO_list_post.append(local_EO_4)\n",
    "local_EO_5=EO_compuatation(y_tilde_5,vali_y_5,vali_s_5)\n",
    "local_EO_list_post.append(local_EO_5)\n",
    "local_EO_avg_post=sum(local_EO_list_post)/len(local_EO_list_post)\n",
    "print('local_EO=',local_EO_list_post)\n",
    "local_EO_avg_post=sum(local_EO_list_post)/len(local_EO_list_post)\n",
    "print('local_EO_avg_post=',local_EO_avg_post)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "07952e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_acc_post= 0.6813333333333333\n"
     ]
    }
   ],
   "source": [
    "acc_count=np.where(vali_tiled_y==vali_y,1,0)\n",
    "acc=sum(acc_count)/len(vali_y)\n",
    "print('global_acc_post=',acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32ceaae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run():\n",
    "#     local_list=[]\n",
    "#     global_eo_list=[]\n",
    "#     acc_list=[]\n",
    "#     for i in range (5):\n",
    "#         e_0=0.03*i\n",
    "#         e_c=1\n",
    "#         x=LP_EO(e_0,e_c)\n",
    "#         beta=np.zeros((5,2,5))\n",
    "#         for c in range(5):\n",
    "#                 for a in range(2):\n",
    "#                     C=[0,0,0,0,0]\n",
    "#                     A=np.array([[1,1,1,1,1],[TP[c,a,0],1,0,0,0],[TP[c,a,1],0,1,0,0],[TP[c,a,2],0,0,1,0],[TP[c,a,3],0,0,0,1]])\n",
    "#                     b=np.array([1,x[c,a,0],x[c,a,1],x[c,a,2],x[c,a,3]])\n",
    "#                     b=np.reshape(b,(5,1))\n",
    "#                     res = linprog(C, A_eq=A, b_eq=b, bounds=(0,1))\n",
    "#                     beta_ac=res.x\n",
    "#                     beta[c,a,:]=beta_ac\n",
    "#                     e=np.matmul(A,beta_ac)\n",
    "#                     print(e)\n",
    "#                     print(b)\n",
    "#         beta=np.reshape(beta,(5,2,5))\n",
    "#         y_tilde_1=[]\n",
    "#         y_tilde_2=[]\n",
    "#         y_tilde_3=[]\n",
    "#         y_tilde_4=[]\n",
    "#         y_tilde_5=[]\n",
    "\n",
    "#         # Example usage\n",
    "#         for c, (feature, label, sensitive) in enumerate([(vali_x_1, vali_y_1, vali_s_1), (vali_x_2, vali_y_2, vali_s_2), (vali_x_3, vali_y_3, vali_s_3), (vali_x_4, vali_y_4, vali_s_4), (vali_x_5, vali_y_5, vali_s_5)]):\n",
    "#             y_pred=global_model.predict(feature)\n",
    "#             y_pred=tf.argmax(y_pred, axis=1)\n",
    "#             for i in range (len(y_pred)):\n",
    "#                 a=sensitive[i]\n",
    "#                 y_hat=y_pred[i]\n",
    "#                 y_tilde=compute_tilde_Y(c,a,y_hat)\n",
    "#                 if c==0:\n",
    "#                     y_tilde_1.append(y_tilde)\n",
    "#                 elif c==1:\n",
    "#                     y_tilde_2.append(y_tilde)\n",
    "#                 elif c==2:\n",
    "#                     y_tilde_3.append(y_tilde)\n",
    "#                 elif c==3:\n",
    "#                     y_tilde_4.append(y_tilde)\n",
    "#                 elif c==4:\n",
    "#                     y_tilde_5.append(y_tilde)\n",
    "#         vali_tiled_y=np.concatenate((y_tilde_1,y_tilde_2,y_tilde_3,y_tilde_4,y_tilde_5),axis=0)\n",
    "#         vali_tiled_y=np.reshape(vali_tiled_y,(-1,))\n",
    "#         global_EO=EO_compuatation(vali_tiled_y,vali_y,vali_s)\n",
    "#         print('global_EO=',global_EO)\n",
    "#         global_eo_list.append(global_EO)\n",
    "#         local_EO_list_post=[]\n",
    "#         local_EO_1=EO_compuatation(y_tilde_1,vali_y_1,vali_s_1)\n",
    "#         local_EO_list_post.append(local_EO_1)\n",
    "#         local_EO_2=EO_compuatation(y_tilde_2,vali_y_2,vali_s_2)\n",
    "#         local_EO_list_post.append(local_EO_2)\n",
    "#         local_EO_3=EO_compuatation(y_tilde_3,vali_y_3,vali_s_3)\n",
    "#         local_EO_list_post.append(local_EO_3)\n",
    "#         local_EO_4=EO_compuatation(y_tilde_4,vali_y_4,vali_s_4)\n",
    "#         local_EO_list_post.append(local_EO_4)\n",
    "#         local_EO_5=EO_compuatation(y_tilde_5,vali_y_5,vali_s_5)\n",
    "#         local_EO_list_post.append(local_EO_5)\n",
    "#         local_EO_avg_post=sum(local_EO_list_post)/len(local_EO_list_post)\n",
    "#         local_EO_avg_post=sum(local_EO_list_post)/len(local_EO_list_post)\n",
    "#         print('local_EO_avg_post=',local_EO_avg_post)\n",
    "#         local_list.append(local_EO_avg_post)\n",
    "#         acc_count=np.where(vali_tiled_y==vali_y,1,0)\n",
    "#         acc=sum(acc_count)/len(vali_y)\n",
    "#         acc_list.append(acc)\n",
    "#         print('global_acc_post=',acc)\n",
    "#     return local_list,global_eo_list,acc_list\n",
    "\n",
    "# # Initialize accumulators for the sum of values\n",
    "# sum_local_list = None\n",
    "# sum_global_eo_list = None\n",
    "# sum_acc_list = None\n",
    "\n",
    "# # Perform 5 runs and accumulate the results\n",
    "# for _ in range(5):\n",
    "#     local_list, global_eo_list, acc_list = run()\n",
    "\n",
    "#     if sum_local_list is None:\n",
    "#         # Initialize accumulators with the first run's values\n",
    "#         sum_local_list = np.array(local_list)\n",
    "#         sum_global_eo_list = np.array(global_eo_list)\n",
    "#         sum_acc_list = np.array(acc_list)\n",
    "#     else:\n",
    "#         # Accumulate the values from subsequent runs\n",
    "#         sum_local_list += np.array(local_list)\n",
    "#         sum_global_eo_list += np.array(global_eo_list)\n",
    "#         sum_acc_list += np.array(acc_list)\n",
    "\n",
    "# # Compute the average by dividing by the number of runs (5 in this case)\n",
    "# average_local_list = sum_local_list / 5\n",
    "# average_global_eo_list = sum_global_eo_list / 5\n",
    "# average_acc_list = sum_acc_list / 5\n",
    "\n",
    "# # Print the average lists\n",
    "# print(\"Average local_list=\", average_local_list)\n",
    "# print(\"Average global_eo_list=\", average_global_eo_list)\n",
    "# print(\"Average acc_list=\", average_acc_list)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336a3ab1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

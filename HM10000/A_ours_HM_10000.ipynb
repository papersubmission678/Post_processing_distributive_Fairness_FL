{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_AUGMENTATION=True\n",
    "AUGMENTATION_RATIO=1\n",
    "batch_size=32\n",
    "E=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 16:47:32.687106: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-21 16:47:32.709970: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-21 16:47:32.709991: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-21 16:47:32.710005: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-21 16:47:32.714381: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-21 16:47:32.714772: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-21 16:47:33.399203: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# from tensorflow.keras.applications import DenseNet121,VGG16\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import random\n",
    "import tensorflow as tf\n",
    "# from data_processing import data_processing\n",
    "from data_2 import data_simulate_noniid\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x,train_y,train_s, vali_x, vali_y, vali_s, train_x_1, vali_x_1, train_y_1, vali_y_1, train_s_1, vali_s_1, train_x_2, vali_x_2, train_y_2, vali_y_2, train_s_2, vali_s_2, train_x_3, vali_x_3, train_y_3, vali_y_3, train_s_3, vali_s_3, train_x_4, vali_x_4, train_y_4, vali_y_4, train_s_4, vali_s_4, train_x_5, vali_x_5, train_y_5, vali_y_5, train_s_5, vali_s_5=data_processing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1, 0.3, 0.5, 0.7, 0.9]\n",
      "(750, 2352) (750, 2352) (750, 2352) (750, 2352) (749, 2352)\n"
     ]
    }
   ],
   "source": [
    "train_x,train_y,train_s, vali_x, vali_y, vali_s, train_x_1, vali_x_1, train_y_1, vali_y_1, train_s_1, vali_s_1, train_x_2, vali_x_2, train_y_2, vali_y_2, train_s_2, vali_s_2, train_x_3, vali_x_3, train_y_3, vali_y_3, train_s_3, vali_s_3, train_x_4, vali_x_4, train_y_4, vali_y_4, train_s_4, vali_s_4, train_x_5, vali_x_5, train_y_5, vali_y_5, train_s_5, vali_s_5=data_simulate_noniid(settings=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense, MaxPool2D, Input, MaxPooling2D, GlobalAveragePooling2D, Dropout\n",
    "import tensorflow as tf\n",
    "def create_CNN_model(): \n",
    "    model = Sequential([\n",
    "        Input(shape=(28, 28, 3)),\n",
    "        Conv2D(32, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(4, activation='softmax')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare  data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images in training dataset before augmentation: 750\n",
      "Images in training dataset after augmentation: 1500\n",
      "Images in training dataset before augmentation: 750\n",
      "Images in training dataset after augmentation: 1500\n",
      "Images in training dataset before augmentation: 750\n",
      "Images in training dataset after augmentation: 1500\n",
      "Images in training dataset before augmentation: 750\n",
      "Images in training dataset after augmentation: 1500\n",
      "Images in training dataset before augmentation: 749\n",
      "Images in training dataset after augmentation: 1498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 16:47:35.773007: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-21 16:47:35.789894: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2211] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "def preprocess_dataset(dataset, is_training):\n",
    "    dataset = dataset.cache().shuffle(10, reshuffle_each_iteration = False)\n",
    "    \n",
    "    if is_training == True and IMAGE_AUGMENTATION == True:\n",
    "        print(\"Images in training dataset before augmentation: \" + str(len(dataset)))\n",
    "        dataset_augmented = dataset.take(int(AUGMENTATION_RATIO*len(dataset))).map(augment, num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
    "        dataset = dataset.concatenate(dataset_augmented)\n",
    "        print(\"Images in training dataset after augmentation: \" + str(len(dataset)))\n",
    "\n",
    "    dataset = dataset.batch(batch_size).prefetch(buffer_size = tf.data.AUTOTUNE)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def augment(image, label_y):\n",
    "    # image = tf.image.rotate(image, random.uniform(-10, 10)*math.pi/180)\n",
    "    image = tf.image.central_crop(image, random.uniform(0.9, 1.0))\n",
    "    image = tf.image.random_brightness(image, max_delta = 0.1)\n",
    "    image = tf.image.random_contrast(image, lower = 0.9, upper = 1.1)\n",
    "    image = tf.image.resize(image, [28, 28])\n",
    "    \n",
    "    return image, label_y\n",
    "def data_augmentation(train_x,train_y):\n",
    "    training_set = tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
    "    training_set = preprocess_dataset(training_set, is_training = True)\n",
    "    return training_set\n",
    "# def data_augmentation(train_x, train_y):\n",
    "#     dataset=tf.data.Dataset.from_tensor_slices((train_x, train_y)).batch(batch_size=batch_size)\n",
    "#     return dataset\n",
    "training_set_1=data_augmentation(train_x_1, train_y_1)\n",
    "training_set_2=data_augmentation(train_x_2, train_y_2)\n",
    "training_set_3=data_augmentation(train_x_3, train_y_3)\n",
    "training_set_4=data_augmentation(train_x_4, train_y_4)\n",
    "training_set_5=data_augmentation(train_x_5, train_y_5)\n",
    "p_1=len(training_set_1)/(len(training_set_1)+len(training_set_2)+len(training_set_3)+len(training_set_4)+len(training_set_5))\n",
    "p_2=len(training_set_2)/(len(training_set_1)+len(training_set_2)+len(training_set_3)+len(training_set_4)+len(training_set_5))\n",
    "p_3=len(training_set_3)/(len(training_set_1)+len(training_set_2)+len(training_set_3)+len(training_set_4)+len(training_set_5))\n",
    "p_4=len(training_set_4)/(len(training_set_1)+len(training_set_2)+len(training_set_3)+len(training_set_4)+len(training_set_5))\n",
    "p_5=len(training_set_5)/(len(training_set_1)+len(training_set_2)+len(training_set_3)+len(training_set_4)+len(training_set_5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense, MaxPool2D, Input, MaxPooling2D, GlobalAveragePooling2D, Dropout\n",
    "import tensorflow as tf\n",
    "def create_CNN_model(): \n",
    "    model = Sequential([\n",
    "        Input(shape=(28, 28, 3)),\n",
    "        Conv2D(32, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(4, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def multiply(weights,c):\n",
    "\n",
    "    new_weights = []\n",
    "    for i in range(len(weights)):\n",
    "        new_weights.append(weights[i]*c)\n",
    "    return new_weights\n",
    "\n",
    "# def norm_square(weights_n):\n",
    "#     sum = 0\n",
    "#     for i in range(len(weights_n)):\n",
    "#         sum += np.linalg.norm(weights_n[i])**2\n",
    "#     return sum\n",
    "def model_weight_aggregation(weight_1, weight_2, weight_3, weight_4, weight_5):\n",
    "    added_weight = []\n",
    "    weight_1=multiply(weight_1,p_1)\n",
    "    weight_2=multiply(weight_2,p_2)\n",
    "    weight_3=multiply(weight_3,p_3)\n",
    "    weight_4=multiply(weight_4,p_4)\n",
    "    weight_5=multiply(weight_5,p_5)\n",
    "    for i in range(len(weight_1)):\n",
    "        added_weight.append(weight_1[i]+weight_2[i]+weight_3[i]+weight_4[i]+weight_5[i])\n",
    "    return added_weight\n",
    "def model_weight_minus(weight_1, weight_2):\n",
    "\n",
    "    minus_weights = []\n",
    "    for i in range(len(weight_1)):\n",
    "        minus_weights.append(weight_1[i] - weight_2[i])\n",
    "    return minus_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a FedAvg model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients_model=create_CNN_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_model=create_CNN_model()\n",
    "w_0=global_model.get_weights()\n",
    "global_weight=[]\n",
    "global_weight.append(w_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(training_set,loacl_epoch):\n",
    "    w_t=global_weight[0]\n",
    "    clients_model.set_weights(w_t)\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "    clients_model.compile(loss = 'sparse_categorical_crossentropy',\n",
    "                  optimizer =optimizer,\n",
    "                  metrics = ['accuracy'])\n",
    "    history =clients_model.fit(training_set,\n",
    "                    epochs = 5)\n",
    "    return clients_model.get_weights()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Fedavg():\n",
    "    w_t_1=train_model(training_set_1,E)\n",
    "    w_t_2=train_model(training_set_2,E)\n",
    "    w_t_3=train_model(training_set_3,E)\n",
    "    w_t_4=train_model(training_set_4,E)\n",
    "    w_t_5=train_model(training_set_5,E)\n",
    "    new_w=model_weight_aggregation(w_t_1,w_t_2,w_t_3,w_t_4,w_t_5)\n",
    "    global_weight[0]=new_w\n",
    "    global_model.set_weights(new_w)\n",
    "    return new_w\n",
    "def acc_calculator(y_true, y_pred):\n",
    "    accuracy_count=np.where(y_true==y_pred, 1.0, 0.0)\n",
    "    accuracy=sum(accuracy_count)/len(y_true)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.8902 - accuracy: 0.6933\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7791 - accuracy: 0.6933\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7601 - accuracy: 0.6933\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7512 - accuracy: 0.6933\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7395 - accuracy: 0.6933\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.8485 - accuracy: 0.7187\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7685 - accuracy: 0.7187\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7416 - accuracy: 0.7187\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7353 - accuracy: 0.7187\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7251 - accuracy: 0.7187\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.8817 - accuracy: 0.6867\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7867 - accuracy: 0.6867\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7740 - accuracy: 0.6867\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7570 - accuracy: 0.6867\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7477 - accuracy: 0.6867\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.9447 - accuracy: 0.6440\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.8452 - accuracy: 0.6440\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.8188 - accuracy: 0.6440\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.8107 - accuracy: 0.6440\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7898 - accuracy: 0.6433\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.8923 - accuracy: 0.6742\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7833 - accuracy: 0.6749\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7593 - accuracy: 0.6742\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7496 - accuracy: 0.6742\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7437 - accuracy: 0.6742\n",
      "9.513199090957642\n",
      "vali_acc= 0.6658666666666667\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.7487 - accuracy: 0.6980\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7243 - accuracy: 0.7047\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6895 - accuracy: 0.7060\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6718 - accuracy: 0.7027\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6429 - accuracy: 0.7253\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.7382 - accuracy: 0.7187\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7237 - accuracy: 0.7187\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7078 - accuracy: 0.7187\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7016 - accuracy: 0.7227\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6916 - accuracy: 0.7300\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.7602 - accuracy: 0.6867\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7491 - accuracy: 0.6867\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7296 - accuracy: 0.6900\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7138 - accuracy: 0.6947\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6982 - accuracy: 0.7053\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 7ms/step - loss: 0.8004 - accuracy: 0.6440\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.7693 - accuracy: 0.6480\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7396 - accuracy: 0.6640\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7135 - accuracy: 0.6807\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6989 - accuracy: 0.6920\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.7485 - accuracy: 0.6702\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7326 - accuracy: 0.6749\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7184 - accuracy: 0.6709\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.7054 - accuracy: 0.6762\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6982 - accuracy: 0.6829\n",
      "9.427008628845215\n",
      "vali_acc= 0.6658666666666667\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.6799 - accuracy: 0.7213\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6458 - accuracy: 0.7247\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6295 - accuracy: 0.7320\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6229 - accuracy: 0.7367\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6081 - accuracy: 0.7373\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.6872 - accuracy: 0.7260\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6657 - accuracy: 0.7320\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.6522 - accuracy: 0.7367\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.6402 - accuracy: 0.7433\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6273 - accuracy: 0.7460\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.7020 - accuracy: 0.6927\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6879 - accuracy: 0.7033\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6654 - accuracy: 0.7140\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6596 - accuracy: 0.7200\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6379 - accuracy: 0.7287\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.7200 - accuracy: 0.6847\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6829 - accuracy: 0.6980\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6712 - accuracy: 0.7067\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6531 - accuracy: 0.7140\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6463 - accuracy: 0.7140\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.6952 - accuracy: 0.6816\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6814 - accuracy: 0.6963\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6721 - accuracy: 0.7016\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6492 - accuracy: 0.7063\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6379 - accuracy: 0.7163\n",
      "9.658012390136719\n",
      "vali_acc= 0.6882666666666667\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.6328 - accuracy: 0.7393\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6143 - accuracy: 0.7387\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6001 - accuracy: 0.7460\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5862 - accuracy: 0.7500\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5680 - accuracy: 0.7693\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.6570 - accuracy: 0.7360\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6285 - accuracy: 0.7513\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6175 - accuracy: 0.7553\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5999 - accuracy: 0.7680\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5896 - accuracy: 0.7693\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.6491 - accuracy: 0.7247\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6424 - accuracy: 0.7140\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6235 - accuracy: 0.7313\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6125 - accuracy: 0.7360\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6031 - accuracy: 0.7373\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.6672 - accuracy: 0.7087\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6337 - accuracy: 0.7207\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6197 - accuracy: 0.7207\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6138 - accuracy: 0.7287\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5924 - accuracy: 0.7373\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.6527 - accuracy: 0.7069\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6293 - accuracy: 0.7216\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6118 - accuracy: 0.7323\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6075 - accuracy: 0.7410\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5940 - accuracy: 0.7363\n",
      "9.119621515274048\n",
      "vali_acc= 0.7170666666666666\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.6016 - accuracy: 0.7380\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5821 - accuracy: 0.7560\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5649 - accuracy: 0.7660\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5592 - accuracy: 0.7647\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5600 - accuracy: 0.7693\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.6302 - accuracy: 0.7513\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.5963 - accuracy: 0.7640\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.5862 - accuracy: 0.7733\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.5718 - accuracy: 0.7773\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.5647 - accuracy: 0.7767\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 7ms/step - loss: 0.6096 - accuracy: 0.7473\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5807 - accuracy: 0.7473\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5681 - accuracy: 0.7533\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.5516 - accuracy: 0.7673\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5462 - accuracy: 0.7613\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.6342 - accuracy: 0.7273\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.6110 - accuracy: 0.7380\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5961 - accuracy: 0.7347\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5843 - accuracy: 0.7367\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5753 - accuracy: 0.7493\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.6195 - accuracy: 0.7276\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5966 - accuracy: 0.7316\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5769 - accuracy: 0.7423\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5654 - accuracy: 0.7457\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5622 - accuracy: 0.7403\n",
      "9.716668367385864\n",
      "vali_acc= 0.7453333333333333\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5830 - accuracy: 0.7513\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5637 - accuracy: 0.7607\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5570 - accuracy: 0.7667\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5377 - accuracy: 0.7767\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5370 - accuracy: 0.7760\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.6253 - accuracy: 0.7553\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5816 - accuracy: 0.7713\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5708 - accuracy: 0.7787\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5616 - accuracy: 0.7813\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5483 - accuracy: 0.7813\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5828 - accuracy: 0.7580\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5518 - accuracy: 0.7620\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5414 - accuracy: 0.7653\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5252 - accuracy: 0.7753\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5198 - accuracy: 0.7847\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.6184 - accuracy: 0.7287\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5920 - accuracy: 0.7453\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5668 - accuracy: 0.7527\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5578 - accuracy: 0.7527\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5581 - accuracy: 0.7553\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.6042 - accuracy: 0.7243\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5738 - accuracy: 0.7403\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5541 - accuracy: 0.7523\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5445 - accuracy: 0.7557\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5309 - accuracy: 0.7650\n",
      "9.248059749603271\n",
      "vali_acc= 0.7490666666666667\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 7ms/step - loss: 0.5669 - accuracy: 0.7607\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5529 - accuracy: 0.7713\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5329 - accuracy: 0.7707\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5231 - accuracy: 0.7807\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5175 - accuracy: 0.7893\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 7ms/step - loss: 0.6097 - accuracy: 0.7573\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5657 - accuracy: 0.7780\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5504 - accuracy: 0.7867\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5412 - accuracy: 0.7920\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5287 - accuracy: 0.7847\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5682 - accuracy: 0.7567\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5250 - accuracy: 0.7747\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5224 - accuracy: 0.7753\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5140 - accuracy: 0.7827\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.5047 - accuracy: 0.7947\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5992 - accuracy: 0.7367\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5734 - accuracy: 0.7540\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5508 - accuracy: 0.7633\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5491 - accuracy: 0.7620\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5289 - accuracy: 0.7707\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5870 - accuracy: 0.7403\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5517 - accuracy: 0.7603\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5350 - accuracy: 0.7623\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5192 - accuracy: 0.7690\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5053 - accuracy: 0.7724\n",
      "9.487669706344604\n",
      "vali_acc= 0.7541333333333333\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5541 - accuracy: 0.7647\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5276 - accuracy: 0.7787\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5162 - accuracy: 0.7767\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5110 - accuracy: 0.7833\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4885 - accuracy: 0.8007\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.6053 - accuracy: 0.7600\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5607 - accuracy: 0.7787\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5496 - accuracy: 0.7927\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5275 - accuracy: 0.7880\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5214 - accuracy: 0.8020\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 7ms/step - loss: 0.5574 - accuracy: 0.7673\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5280 - accuracy: 0.7727\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5141 - accuracy: 0.7840\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.5155 - accuracy: 0.7833\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5159 - accuracy: 0.7747\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 7ms/step - loss: 0.5850 - accuracy: 0.7467\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.5506 - accuracy: 0.7627\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.5379 - accuracy: 0.7667\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5264 - accuracy: 0.7767\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.5219 - accuracy: 0.7893\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 7ms/step - loss: 0.5748 - accuracy: 0.7403\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5374 - accuracy: 0.7603\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5218 - accuracy: 0.7644\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4942 - accuracy: 0.7784\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4903 - accuracy: 0.7817\n",
      "9.649636030197144\n",
      "vali_acc= 0.7621333333333333\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5477 - accuracy: 0.7687\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5177 - accuracy: 0.7807\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5023 - accuracy: 0.7873\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4719 - accuracy: 0.8080\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4506 - accuracy: 0.8193\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5925 - accuracy: 0.7653\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5520 - accuracy: 0.7853\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5264 - accuracy: 0.7960\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.5210 - accuracy: 0.7920\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.5131 - accuracy: 0.7927\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5475 - accuracy: 0.7653\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5148 - accuracy: 0.7787\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5088 - accuracy: 0.7793\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5200 - accuracy: 0.7747\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.5043 - accuracy: 0.7873\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 7ms/step - loss: 0.5771 - accuracy: 0.7473\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5402 - accuracy: 0.7680\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5252 - accuracy: 0.7727\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5071 - accuracy: 0.7947\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4937 - accuracy: 0.8033\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5583 - accuracy: 0.7443\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5101 - accuracy: 0.7744\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5005 - accuracy: 0.7790\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4734 - accuracy: 0.7870\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4684 - accuracy: 0.7971\n",
      "9.496973514556885\n",
      "vali_acc= 0.7629333333333334\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5167 - accuracy: 0.7793\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5041 - accuracy: 0.7907\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4787 - accuracy: 0.8013\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4511 - accuracy: 0.8173\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4462 - accuracy: 0.8120\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5765 - accuracy: 0.7720\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5336 - accuracy: 0.7893\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5234 - accuracy: 0.7867\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5038 - accuracy: 0.8027\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4836 - accuracy: 0.8073\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5422 - accuracy: 0.7720\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5113 - accuracy: 0.7900\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5123 - accuracy: 0.7680\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4959 - accuracy: 0.7860\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4984 - accuracy: 0.7860\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5751 - accuracy: 0.7500\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5266 - accuracy: 0.7873\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5080 - accuracy: 0.7900\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4928 - accuracy: 0.7960\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4686 - accuracy: 0.8133\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5523 - accuracy: 0.7537\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4965 - accuracy: 0.7770\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4896 - accuracy: 0.7830\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4696 - accuracy: 0.7797\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4678 - accuracy: 0.7931\n",
      "9.222609996795654\n",
      "vali_acc= 0.7666666666666667\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5179 - accuracy: 0.7793\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4978 - accuracy: 0.7887\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4719 - accuracy: 0.8073\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4425 - accuracy: 0.8233\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4197 - accuracy: 0.8233\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5659 - accuracy: 0.7720\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5133 - accuracy: 0.7867\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5025 - accuracy: 0.7960\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4809 - accuracy: 0.8053\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4743 - accuracy: 0.8167\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5398 - accuracy: 0.7727\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.4968 - accuracy: 0.7960\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.4926 - accuracy: 0.7867\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.4778 - accuracy: 0.7860\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.4662 - accuracy: 0.7987\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5515 - accuracy: 0.7587\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5004 - accuracy: 0.7900\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4867 - accuracy: 0.7980\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.4606 - accuracy: 0.8087\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.4519 - accuracy: 0.8113\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5376 - accuracy: 0.7623\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.4851 - accuracy: 0.7891\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4618 - accuracy: 0.7997\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.4459 - accuracy: 0.8064\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.4277 - accuracy: 0.8064\n",
      "10.092528343200684\n",
      "vali_acc= 0.7658666666666667\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 7ms/step - loss: 0.4915 - accuracy: 0.8007\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4680 - accuracy: 0.8087\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.4366 - accuracy: 0.8220\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.4228 - accuracy: 0.8267\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.4093 - accuracy: 0.8247\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5462 - accuracy: 0.7793\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4977 - accuracy: 0.8007\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4848 - accuracy: 0.7993\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4663 - accuracy: 0.8093\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4473 - accuracy: 0.8187\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5323 - accuracy: 0.7707\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4779 - accuracy: 0.7953\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4593 - accuracy: 0.8053\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4471 - accuracy: 0.8053\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4468 - accuracy: 0.8087\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5328 - accuracy: 0.7720\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4845 - accuracy: 0.8040\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4701 - accuracy: 0.8020\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4507 - accuracy: 0.8147\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4338 - accuracy: 0.8280\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5148 - accuracy: 0.7757\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4674 - accuracy: 0.8017\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4421 - accuracy: 0.8031\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4301 - accuracy: 0.8158\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4060 - accuracy: 0.8204\n",
      "9.404434204101562\n",
      "vali_acc= 0.7738666666666667\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.4852 - accuracy: 0.7973\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4420 - accuracy: 0.8193\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4178 - accuracy: 0.8320\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3920 - accuracy: 0.8440\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3985 - accuracy: 0.8427\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5318 - accuracy: 0.7880\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4777 - accuracy: 0.8013\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4622 - accuracy: 0.8093\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4565 - accuracy: 0.8113\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4699 - accuracy: 0.8107\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5302 - accuracy: 0.7700\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4657 - accuracy: 0.7947\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4544 - accuracy: 0.8100\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4277 - accuracy: 0.8193\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4088 - accuracy: 0.8260\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5179 - accuracy: 0.7840\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4794 - accuracy: 0.8000\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4521 - accuracy: 0.8067\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4215 - accuracy: 0.8280\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4139 - accuracy: 0.8387\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5044 - accuracy: 0.7817\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4433 - accuracy: 0.8064\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4205 - accuracy: 0.8138\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4094 - accuracy: 0.8344\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3841 - accuracy: 0.8418\n",
      "9.070215225219727\n",
      "vali_acc= 0.768\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.4618 - accuracy: 0.8060\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4465 - accuracy: 0.8240\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4061 - accuracy: 0.8387\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3823 - accuracy: 0.8473\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3665 - accuracy: 0.8673\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5243 - accuracy: 0.8000\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4738 - accuracy: 0.8087\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4576 - accuracy: 0.8147\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.4344 - accuracy: 0.8300\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.4103 - accuracy: 0.8400\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5088 - accuracy: 0.7820\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4592 - accuracy: 0.8053\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4227 - accuracy: 0.8240\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4099 - accuracy: 0.8347\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3940 - accuracy: 0.8340\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 7ms/step - loss: 0.5011 - accuracy: 0.7927\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.4538 - accuracy: 0.8207\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.4171 - accuracy: 0.8340\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3968 - accuracy: 0.8367\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.3785 - accuracy: 0.8507\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 7ms/step - loss: 0.4866 - accuracy: 0.7857\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4371 - accuracy: 0.8051\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4075 - accuracy: 0.8231\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3846 - accuracy: 0.8358\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3711 - accuracy: 0.8398\n",
      "9.523167848587036\n",
      "vali_acc= 0.7738666666666667\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 7ms/step - loss: 0.4437 - accuracy: 0.8200\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4305 - accuracy: 0.8240\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3955 - accuracy: 0.8500\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3619 - accuracy: 0.8613\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3830 - accuracy: 0.8507\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5142 - accuracy: 0.7993\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4589 - accuracy: 0.8140\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4355 - accuracy: 0.8273\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.4228 - accuracy: 0.8347\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3897 - accuracy: 0.8447\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.4951 - accuracy: 0.7887\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4199 - accuracy: 0.8173\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4056 - accuracy: 0.8307\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3793 - accuracy: 0.8407\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3621 - accuracy: 0.8493\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.4929 - accuracy: 0.7960\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4321 - accuracy: 0.8247\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3993 - accuracy: 0.8380\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3824 - accuracy: 0.8473\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3507 - accuracy: 0.8700\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.4701 - accuracy: 0.7937\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4105 - accuracy: 0.8151\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3826 - accuracy: 0.8438\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3702 - accuracy: 0.8465\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3348 - accuracy: 0.8672\n",
      "8.952334642410278\n",
      "vali_acc= 0.7752\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.4308 - accuracy: 0.8200\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4113 - accuracy: 0.8233\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3668 - accuracy: 0.8640\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3671 - accuracy: 0.8573\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3371 - accuracy: 0.8687\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5194 - accuracy: 0.8027\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4335 - accuracy: 0.8280\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4182 - accuracy: 0.8273\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4055 - accuracy: 0.8447\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3757 - accuracy: 0.8500\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.4871 - accuracy: 0.7933\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4129 - accuracy: 0.8260\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3918 - accuracy: 0.8447\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3780 - accuracy: 0.8480\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3550 - accuracy: 0.8480\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.4710 - accuracy: 0.8107\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4154 - accuracy: 0.8347\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3847 - accuracy: 0.8487\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3588 - accuracy: 0.8573\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3386 - accuracy: 0.8753\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.4664 - accuracy: 0.7937\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3885 - accuracy: 0.8418\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3744 - accuracy: 0.8411\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3581 - accuracy: 0.8425\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3342 - accuracy: 0.8565\n",
      "9.456412315368652\n",
      "vali_acc= 0.7773333333333333\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.4271 - accuracy: 0.8253\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3803 - accuracy: 0.8447\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3616 - accuracy: 0.8580\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3641 - accuracy: 0.8567\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3499 - accuracy: 0.8633\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.5074 - accuracy: 0.8060\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4080 - accuracy: 0.8273\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.3789 - accuracy: 0.8420\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3666 - accuracy: 0.8493\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3551 - accuracy: 0.8567\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.4902 - accuracy: 0.7973\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4258 - accuracy: 0.8220\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3945 - accuracy: 0.8413\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3802 - accuracy: 0.8453\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3567 - accuracy: 0.8527\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.4602 - accuracy: 0.8087\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.4013 - accuracy: 0.8347\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3581 - accuracy: 0.8620\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3419 - accuracy: 0.8693\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3208 - accuracy: 0.8820\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.4501 - accuracy: 0.8017\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3951 - accuracy: 0.8258\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3527 - accuracy: 0.8451\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3304 - accuracy: 0.8678\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3198 - accuracy: 0.8638\n",
      "9.194174766540527\n",
      "vali_acc= 0.7733333333333333\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.4215 - accuracy: 0.8240\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3610 - accuracy: 0.8500\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3662 - accuracy: 0.8473\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3669 - accuracy: 0.8560\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3731 - accuracy: 0.8547\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.4863 - accuracy: 0.8113\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3987 - accuracy: 0.8267\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3651 - accuracy: 0.8473\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3520 - accuracy: 0.8540\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3425 - accuracy: 0.8633\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.4624 - accuracy: 0.8100\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3796 - accuracy: 0.8340\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3691 - accuracy: 0.8440\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3521 - accuracy: 0.8533\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3260 - accuracy: 0.8640\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.4545 - accuracy: 0.8120\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3753 - accuracy: 0.8580\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3522 - accuracy: 0.8593\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3272 - accuracy: 0.8767\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3047 - accuracy: 0.8827\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.4482 - accuracy: 0.7937\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3632 - accuracy: 0.8331\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3485 - accuracy: 0.8418\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3262 - accuracy: 0.8545\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3105 - accuracy: 0.8765\n",
      "9.16161823272705\n",
      "vali_acc= 0.7741333333333333\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.3910 - accuracy: 0.8400\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3516 - accuracy: 0.8587\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3315 - accuracy: 0.8660\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3429 - accuracy: 0.8553\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.3672 - accuracy: 0.8460\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 7ms/step - loss: 0.4617 - accuracy: 0.8180\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3943 - accuracy: 0.8347\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3550 - accuracy: 0.8713\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3264 - accuracy: 0.8660\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3035 - accuracy: 0.8833\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.4588 - accuracy: 0.8060\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3714 - accuracy: 0.8540\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3352 - accuracy: 0.8687\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3259 - accuracy: 0.8667\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.3058 - accuracy: 0.8780\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.4484 - accuracy: 0.8187\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3695 - accuracy: 0.8520\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3169 - accuracy: 0.8827\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.2974 - accuracy: 0.8947\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.2776 - accuracy: 0.8947\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 7ms/step - loss: 0.4296 - accuracy: 0.8077\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.3526 - accuracy: 0.8438\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3177 - accuracy: 0.8712\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3165 - accuracy: 0.8652\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.2777 - accuracy: 0.8872\n",
      "9.499359130859375\n",
      "vali_acc= 0.772\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.3994 - accuracy: 0.8367\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3719 - accuracy: 0.8540\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3365 - accuracy: 0.8607\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3438 - accuracy: 0.8633\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.3362 - accuracy: 0.8693\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.4447 - accuracy: 0.8167\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3574 - accuracy: 0.8527\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3300 - accuracy: 0.8627\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3095 - accuracy: 0.8793\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.2868 - accuracy: 0.8880\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.4508 - accuracy: 0.8127\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3557 - accuracy: 0.8553\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3165 - accuracy: 0.8673\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 0.2938 - accuracy: 0.8820\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.2699 - accuracy: 0.8973\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 7ms/step - loss: 0.4152 - accuracy: 0.8233\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.3446 - accuracy: 0.8700\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.2930 - accuracy: 0.8840\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.2725 - accuracy: 0.8953\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.2590 - accuracy: 0.9133\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 0.4221 - accuracy: 0.8164\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3291 - accuracy: 0.8538\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3060 - accuracy: 0.8692\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3062 - accuracy: 0.8785\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.2750 - accuracy: 0.8972\n",
      "9.33960247039795\n",
      "vali_acc= 0.7717333333333334\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "vali_accuracy=[]\n",
    "time_arr=[]\n",
    "for m in range (20):\n",
    "    y_pred=global_model(vali_x)\n",
    "    y_pred=tf.argmax(y_pred, axis=1)\n",
    "    acc=acc_calculator(vali_y,y_pred)\n",
    "    vali_accuracy.append(acc)\n",
    "    start=time.time()\n",
    "    w=Fedavg()\n",
    "    end=time.time()\n",
    "    global_model.set_weights(w)\n",
    "    time_count=end-start\n",
    "    time_arr.append(time_count)\n",
    "    print(end-start)\n",
    "    print('vali_acc=',acc)\n",
    "plt.plot(vali_accuracy,'b--',label='FedAvg')\n",
    "plt.xlabel('# of rounds')\n",
    "plt.ylabel('validation accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "# print('The avg time for per communication'+str(np.mean(time_arr)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy and fairness of FedAvg model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EO_compuatation(y_pred, y_ture,s_sensitive):\n",
    "    EO_list=[]\n",
    "    y_pred=np.reshape(y_pred,(-1,))\n",
    "    y_ture=np.reshape(y_ture,(-1,))\n",
    "    s_sensitive=np.reshape(s_sensitive,(-1,))\n",
    "    for i in range (4):\n",
    "        tpr_sensitive=np.sum(np.where((y_pred==i)&(y_ture==i)&(s_sensitive==1),1,0))/np.sum(np.where((y_ture==i)&(s_sensitive==1),1,0))\n",
    "        tpr_nonsensitive=np.sum(np.where((y_pred==i)&(y_ture==i)&(s_sensitive==0),1,0))/np.sum(np.where((y_ture==i)&(s_sensitive==0),1,0))\n",
    "        EO=abs(tpr_sensitive-tpr_nonsensitive)\n",
    "        if EO<1 or EO==1:\n",
    "            EO_list.append(EO)\n",
    "        else:\n",
    "            EO_list.append(0.01)\n",
    "    print(EO_list)\n",
    "    EO=np.max(EO_list)\n",
    "    return EO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.14225774225774235, 0.12035890416911077, 0.021014790627690294, 0.13236929922135704]\n",
      "global_EO= 0.14225774225774235\n",
      "global_acc= 0.7709333333333334\n"
     ]
    }
   ],
   "source": [
    "y_pred=global_model(vali_x)\n",
    "y_pred=tf.argmax(y_pred, axis=1)\n",
    "vali_EO=EO_compuatation(y_pred, vali_y,vali_s)\n",
    "print('global_EO=',vali_EO)\n",
    "y_pred=global_model(vali_x)\n",
    "y_pred=tf.argmax(y_pred, axis=1)\n",
    "acc_count=np.where(y_pred==vali_y,1,0)\n",
    "acc=sum(acc_count)/len(vali_y)\n",
    "print('global_acc=',acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.18598130841121496, 0.43902439024390244, 0.023925314806773845, 0.5]\n",
      "[0.2090909090909091, 0.046031746031746035, 0.05007573732513593, 0.23333333333333334]\n",
      "[0.17025862068965514, 0.2050183598531212, 0.04320751712056059, 0.25]\n",
      "[0.05711354309165528, 0.027419354838709664, 0.0014376135614879715, 0.05555555555555558]\n",
      "[0.04972875226039786, 0.3058823529411765, 0.03796203796203801, 0.36363636363636365]\n",
      "local_EO= 0.2808166480122705\n",
      "local_EO= [0.5, 0.23333333333333334, 0.25, 0.05711354309165528, 0.36363636363636365]\n"
     ]
    }
   ],
   "source": [
    "vali_x_1=np.reshape(vali_x_1,(-1,28,28,3))\n",
    "vali_x_2=np.reshape(vali_x_2,(-1,28,28,3))\n",
    "vali_x_3=np.reshape(vali_x_3,(-1,28,28,3))\n",
    "vali_x_4=np.reshape(vali_x_4,(-1,28,28,3))\n",
    "vali_x_5=np.reshape(vali_x_5,(-1,28,28,3))\n",
    "\n",
    "local_EO=[]\n",
    "y_pred_1=global_model(vali_x_1)\n",
    "y_pred_1=tf.argmax(y_pred_1, axis=1)\n",
    "EO_1=EO_compuatation(y_pred_1, vali_y_1,vali_s_1)\n",
    "local_EO.append(EO_1)\n",
    "y_pred_2=global_model(vali_x_2)\n",
    "y_pred_2=tf.argmax(y_pred_2, axis=1)\n",
    "EO_2=EO_compuatation(y_pred_2, vali_y_2,vali_s_2)\n",
    "local_EO.append(EO_2)\n",
    "y_pred_3=global_model(vali_x_3)\n",
    "y_pred_3=tf.argmax(y_pred_3, axis=1)\n",
    "EO_3=EO_compuatation(y_pred_3, vali_y_3,vali_s_3)\n",
    "local_EO.append(EO_3)\n",
    "y_pred_4=global_model(vali_x_4)\n",
    "y_pred_4=tf.argmax(y_pred_4, axis=1)\n",
    "EO_4=EO_compuatation(y_pred_4, vali_y_4,vali_s_4)\n",
    "local_EO.append(EO_4)\n",
    "y_pred_5=global_model(vali_x_5)\n",
    "y_pred_5=tf.argmax(y_pred_5, axis=1)\n",
    "EO_5=EO_compuatation(y_pred_5, vali_y_5,vali_s_5)\n",
    "local_EO.append(EO_5)\n",
    "local_EO_avg=sum(local_EO)/len(local_EO)\n",
    "print('local_EO=',local_EO_avg)\n",
    "print('local_EO=',local_EO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare the parameters for LP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_computation(label,sensitive_attribute,y,a):\n",
    "    label=np.reshape(label,(-1,))\n",
    "    sensitive_attribute=np.reshape(sensitive_attribute,(-1,))\n",
    "    N=len(label)\n",
    "    mask=(label==y)&(sensitive_attribute==a)\n",
    "    p_y_a=np.sum(mask)/N\n",
    "    return p_y_a\n",
    "S=np.zeros((5,2,4))\n",
    "p=[len(vali_x_1),len(vali_x_2),len(vali_x_3),len(vali_x_4),len(vali_x_5)]  \n",
    "for c,(feature,labels,sensitive) in enumerate(([vali_x_1,vali_y_1,vali_s_1],[vali_x_2,vali_y_2,vali_s_2],[vali_x_3,vali_y_3,vali_s_3],[vali_x_4,vali_y_4,vali_s_4],[vali_x_5,vali_y_5,vali_s_5])):\n",
    "    for a in range(2):\n",
    "        for y in range(4):\n",
    "            S[c][a][y]=p[c]*p_computation(labels,sensitive,y,a)/(len(vali_x_1)+len(vali_x_2)+len(vali_x_3)+len(vali_x_4)+len(vali_x_5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP=np.zeros((5,2,4))\n",
    "def compute_TP (y_pred,label, sensitive_attribute, y, a):\n",
    "    y_pred=np.reshape(y_pred,(-1,))\n",
    "    label=np.reshape(label,(-1,))\n",
    "    N=len(y_pred)\n",
    "    sensitive_attribute=np.reshape(sensitive_attribute,(-1,))# Total number of samples\n",
    "    # Create a boolean mask for where both conditions are met\n",
    "    count_1= (y_pred == y) & (label == y) & (sensitive_attribute == a)\n",
    "    count_2 = (label== y) & (sensitive_attribute == a)\n",
    "    \n",
    "    # Compute the probability\n",
    "    TP_y_ac = np.sum(count_1) / np.sum(count_2)\n",
    "    if TP_y_ac<1 or TP_y_ac==1:\n",
    "        TP_y_ac=TP_y_ac\n",
    "    else:\n",
    "        TP_y_ac=0\n",
    "    return TP_y_ac\n",
    "for c,(feature,labels,sensitive) in enumerate (([vali_x_1,vali_y_1,vali_s_1],[vali_x_2,vali_y_2,vali_s_2],[vali_x_3,vali_y_3,vali_s_3],[vali_x_4,vali_y_4,vali_s_4],[vali_x_5,vali_y_5,vali_s_5])):\n",
    "    y_pred=global_model(feature)\n",
    "    y_pred=tf.argmax(y_pred, axis=1)\n",
    "    for a in range(2):\n",
    "        for y in range(4):\n",
    "            TP[c][a][y]=compute_TP(y_pred,labels,sensitive,y,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_alpha(label, sensitive_attribute, y, a):\n",
    "    label=np.reshape(label,(-1,))\n",
    "    N=len(label)\n",
    "    sensitive_attribute=np.reshape(sensitive_attribute,(-1,))# Total number of samples\n",
    "    # Create a boolean mask for where both conditions are met\n",
    "    mask = (label == y) & (sensitive_attribute == a)\n",
    "    # print(mask)\n",
    "    # print(\"Number of matching samples:\", np.sum(mask))\n",
    "    \n",
    "    # Compute the probability\n",
    "    alpha_y_ac = np.sum(mask) / N\n",
    "    return alpha_y_ac\n",
    "alpha_a_y=np.zeros((2,4))\n",
    "for a in range(2):\n",
    "    for y in range(4):\n",
    "        alpha_a_y[a][y]=compute_alpha(vali_y,vali_s,y,a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linprog\n",
    "import numpy as np\n",
    "def LP_EO(e_0,e_c):\n",
    "    C=[]\n",
    "    for c in range(5):\n",
    "        for a in range(2):\n",
    "            for y in range(4):\n",
    "                C.append(-S[c][a][y])\n",
    "    ## global fairness constraints\n",
    "    A_1=[]\n",
    "    A_2=[]\n",
    "    A_3=[]\n",
    "    A_4=[]\n",
    "\n",
    "    for c in range(5):\n",
    "        for a in range(2):\n",
    "            for y in range(4):\n",
    "                if a==0 and y==0:\n",
    "                    A_1.append(-S[c][a][y]/alpha_a_y[a][y])\n",
    "                elif a==1 and y==0:\n",
    "                    A_1.append(S[c][a][y]/alpha_a_y[a][y])\n",
    "                else:\n",
    "                    A_1.append(0)\n",
    "    for c in range(5):\n",
    "        for a in range(2):\n",
    "            for y in range(4):\n",
    "                if a==0 and y==1:\n",
    "                    A_2.append(-S[c][a][y]/alpha_a_y[a][y])\n",
    "                elif a==1 and y==1:\n",
    "                    A_2.append(S[c][a][y]/alpha_a_y[a][y])\n",
    "                else:\n",
    "                    A_2.append(0)\n",
    "    for c in range(5):\n",
    "        for a in range(2):\n",
    "            for y in range(4):\n",
    "                if a==0 and y==2:\n",
    "                    A_3.append(-S[c][a][y]/alpha_a_y[a][y])\n",
    "                elif a==1 and y==2:\n",
    "                    A_3.append(S[c][a][y]/alpha_a_y[a][y])\n",
    "                else:\n",
    "                    A_3.append(0)\n",
    "    for c in range(5):\n",
    "        for a in range(2):\n",
    "            for y in range(4):\n",
    "                if a==0 and y==3:\n",
    "                    A_4.append(-S[c][a][y]/alpha_a_y[a][y])\n",
    "                elif a==1 and y==3:\n",
    "                    A_4.append(S[c][a][y]/alpha_a_y[a][y])\n",
    "\n",
    "                else:\n",
    "                    A_4.append(0)\n",
    "    A_1=np.array(A_1)\n",
    "    A_2=np.array(A_2)\n",
    "    A_3=np.array(A_3)\n",
    "    A_4=np.array(A_4)\n",
    "    ## define local fairness constraints\n",
    "    basis_vector=np.eye(4)\n",
    "    zero=np.zeros((4,4))\n",
    "    row_1=np.hstack((basis_vector, -basis_vector, zero, zero, zero, zero, zero, zero, zero, zero))\n",
    "    row_2=np.hstack((zero,zero, basis_vector, -basis_vector, zero, zero, zero, zero, zero, zero))\n",
    "    row_3=np.hstack((zero,zero, zero, zero, basis_vector, -basis_vector, zero, zero, zero, zero))\n",
    "    row_4=np.hstack((zero,zero, zero, zero, zero, zero, basis_vector, -basis_vector, zero, zero))\n",
    "    row_5=np.hstack((zero,zero, zero, zero, zero, zero, zero, zero, basis_vector, -basis_vector))\n",
    "    A_l=np.vstack((row_1,row_2,row_3,row_4,row_5))\n",
    "    def K_ac_compute(a,c):\n",
    "        K_ac = np.zeros((5, 4))  # 5 rows, 4 columns\n",
    "        l_ac = np.zeros((5, 1))  # 5 rows, 1 column\n",
    "\n",
    "        # Modify this line to assign only 4 elements (to match the number of columns in K_ac)\n",
    "        K_ac[0:] = [-1, -1, -1, -1]  # 4 elements, matching the number of columns\n",
    "\n",
    "        # Similarly adjust the rest of the assignments to match the column count of 4\n",
    "        K_ac[1:] = [(1 - (TP[c, a, 1] + TP[c, a, 2] + TP[c, a, 3])), TP[c, a, 0], TP[c, a, 0], TP[c, a, 0]]\n",
    "        K_ac[2:] = [TP[c, a, 1], (1 - (TP[c, a, 0] + TP[c, a, 2] + TP[c, a, 3])), TP[c, a, 1], TP[c, a, 1]]\n",
    "        K_ac[3:] = [TP[c, a, 2], TP[c, a, 2], (1 - (TP[c, a, 0] + TP[c, a, 1] + TP[c, a, 3])), TP[c, a, 2]]\n",
    "        K_ac[4:] = [TP[c, a, 3], TP[c, a, 3], TP[c, a, 3], (1 - (TP[c, a, 0] + TP[c, a, 1] + TP[c, a, 2]))]\n",
    "\n",
    "        l_ac[0:] = [-1]\n",
    "        l_ac[1:] = [TP[c, a, 0]]\n",
    "        l_ac[2:] = [TP[c, a, 1]]\n",
    "        l_ac[3:] = [TP[c, a, 2]]\n",
    "        l_ac[4:] = [TP[c, a, 3]]\n",
    "        return K_ac,l_ac\n",
    "    k_01, l_01 = K_ac_compute(0, 0)\n",
    "    k_11, l_11 = K_ac_compute(1, 0)\n",
    "    k_02, l_02 = K_ac_compute(0, 1)\n",
    "    k_12, l_12 = K_ac_compute(1, 1)\n",
    "    k_03, l_03 = K_ac_compute(0, 2)\n",
    "    k_13, l_13 = K_ac_compute(1, 2)\n",
    "    k_04, l_04 = K_ac_compute(0, 3)\n",
    "    k_14, l_14 = K_ac_compute(1, 3)\n",
    "    k_05, l_05 = K_ac_compute(0, 4)\n",
    "    k_15, l_15 = K_ac_compute(1, 4)\n",
    "\n",
    "\n",
    "    M = np.zeros((50,40))  # 10 blocks of 6x5 matrices; resulting in a 60x25 matrix\n",
    "    l = np.zeros((50, 1))   # Vector to match the 60 rows\n",
    "\n",
    "    # Place each submatrix on the diagonal\n",
    "    M[0:5, 0:4] = k_01    # Place k_01 in the top-left\n",
    "    M[5:10, 4:8] = k_11  # Place k_11 in the next diagonal block\n",
    "    M[10:15, 8:12] = k_02  # Place k_02 in the next diagonal block\n",
    "    M[15:20, 12:16] = k_12  # Place k_12 in the next diagonal block\n",
    "    M[20:25, 16:20] = k_03  # Place k_03 in the next diagonal block\n",
    "    M[25:30, 20:24] = k_13    # Next row of blocks, place k_13\n",
    "    M[30:35, 24:28] = k_04  # Place k_14 in the next block\n",
    "    M[35:40, 28:32] = k_14 # Continue placing the remaining matrices\n",
    "    M[40:45, 32:36] = k_05 # Place k_05\n",
    "    M[45:50, 36:40] = k_15 # Place k_15 in the last block\n",
    "\n",
    "    # Construct vector l\n",
    "    l[0:5] = l_01\n",
    "    l[5:10] = l_11\n",
    "    l[10:15] = l_02\n",
    "    l[15:20] = l_12\n",
    "    l[20:25] = l_03\n",
    "    l[25:30] = l_13\n",
    "    l[30:35] = l_04\n",
    "    l[35:40] = l_14\n",
    "    l[40:45] = l_05\n",
    "    l[45:50] = l_15\n",
    "    A=np.vstack((A_1,A_2,A_3,A_4,A_l,-A_1,-A_2,-A_3,-A_4,-A_l,M))\n",
    "    b_global=e_0*np.ones((4,1))\n",
    "    b_local=e_c*np.ones((20,1))\n",
    "    # print(b_local)\n",
    "    b=np.vstack((b_global,b_local,b_global,b_local,l))\n",
    "    # print(b)\n",
    "    res = linprog(C, A_ub=A, b_ub=b)\n",
    "    x=res.x\n",
    "    x=np.reshape(x,(5,2,4))\n",
    "    # print(x)\n",
    "    # print(res.fun)\n",
    "    return x\n",
    "x=LP_EO(0.01,0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve the LAE and generate the fair prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 2ms/step\n",
      "24/24 [==============================] - 0s 2ms/step\n",
      "24/24 [==============================] - 0s 2ms/step\n",
      "24/24 [==============================] - 0s 2ms/step\n",
      "24/24 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "vali_x_1=np.reshape(vali_x_1,(-1,28,28,3))\n",
    "vali_x_2=np.reshape(vali_x_2,(-1,28,28,3))\n",
    "vali_x_3=np.reshape(vali_x_3,(-1,28,28,3))\n",
    "vali_x_4=np.reshape(vali_x_4,(-1,28,28,3))\n",
    "vali_x_5=np.reshape(vali_x_5,(-1,28,28,3))\n",
    "beta=np.zeros((5,2,5))\n",
    "for c in range(5):\n",
    "        for a in range(2):\n",
    "            C=[0,0,0,0,0]\n",
    "            A=np.array([[1,1,1,1,1],[TP[c,a,0],1,0,0,0],[TP[c,a,1],0,1,0,0],[TP[c,a,2],0,0,1,0],[TP[c,a,3],0,0,0,1]])\n",
    "            b=np.array([1,x[c,a,0],x[c,a,1],x[c,a,2],x[c,a,3]])\n",
    "            b=np.reshape(b,(5,1))\n",
    "            res = linprog(C, A_eq=A, b_eq=b, bounds=(0,1))\n",
    "            beta_ac=res.x\n",
    "            beta[c,a,:]=beta_ac\n",
    "            e=np.matmul(A,beta_ac)\n",
    "beta=np.reshape(beta,(5,2,5))\n",
    "\n",
    "\n",
    "\n",
    "def compute_tilde_Y(c,a, Y_hat,beta):\n",
    "    \"\"\"\n",
    "    Compute \\widetilde{Y}_{\\boldsymbol{\\beta}_{ac}}(x,a,c) based on given probabilities.\n",
    "\n",
    "    Parameters:\n",
    "    - beta_ac: Dictionary with keys 'beta_0' and 'beta_y' for probabilities.\n",
    "    - Y_hat: The predicted value \\hat{Y}(x,a,c).\n",
    "    - y_values: List or array of possible y values in \\mathcal{Y}.\n",
    "\n",
    "    Returns:\n",
    "    - tilde_Y: The computed value of \\widetilde{Y}.\n",
    "    \"\"\"\n",
    "    # Extract probabilities\n",
    "    beta_0 = beta[c, a, 0]  # Probability for Y_hat\n",
    "    beta_ac = beta[c, a, :] \n",
    "    # Probabilities for other y in \\mathcal{Y}\n",
    "    \n",
    "    # Normalize probabilities\n",
    "    \n",
    "    # Generate a random number\n",
    "    rand_val = random.random()\n",
    "    # Determine the output based on random value\n",
    "    if rand_val < beta_0:\n",
    "        return Y_hat\n",
    "    elif rand_val < beta_0 + beta_ac[1]:\n",
    "        return 0\n",
    "    elif rand_val < beta_0 + beta_ac[1] + beta_ac[2]:\n",
    "        return 1\n",
    "    elif rand_val < beta_0 + beta_ac[1] + beta_ac[2] + beta_ac[3]:\n",
    "        return 2\n",
    "    else: # Last value\n",
    "        return 3\n",
    "\n",
    "y_tilde_1=[]\n",
    "y_tilde_2=[]\n",
    "y_tilde_3=[]\n",
    "y_tilde_4=[]\n",
    "y_tilde_5=[]\n",
    "\n",
    "# Example usage\n",
    "for c, (feature, label, sensitive) in enumerate([(vali_x_1, vali_y_1, vali_s_1), (vali_x_2, vali_y_2, vali_s_2), (vali_x_3, vali_y_3, vali_s_3), (vali_x_4, vali_y_4, vali_s_4), (vali_x_5, vali_y_5, vali_s_5)]):\n",
    "    y_pred=global_model.predict(feature)\n",
    "    y_pred=tf.argmax(y_pred, axis=1)\n",
    "    for i in range (len(y_pred)):\n",
    "        a=sensitive[i]\n",
    "        y_hat=y_pred[i]\n",
    "        y_tilde=compute_tilde_Y(c,a,y_hat,beta)\n",
    "        if c==0:\n",
    "            y_tilde_1.append(y_tilde)\n",
    "        elif c==1:\n",
    "            y_tilde_2.append(y_tilde)\n",
    "        elif c==2:\n",
    "            y_tilde_3.append(y_tilde)\n",
    "        elif c==3:\n",
    "            y_tilde_4.append(y_tilde)\n",
    "        elif c==4:\n",
    "            y_tilde_5.append(y_tilde)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.002247752247752255, 0.020182928153989865, 0.0024489463519812293, 0.04560622914349277]\n",
      "global_EO= 0.04560622914349277\n",
      "[0.06915887850467292, 0.07317073170731707, 0.0017368649587493623, 0.16666666666666663]\n",
      "[0.08414376321353065, 0.07936507936507936, 0.026819923371647514, 0.3666666666666667]\n",
      "[0.017241379310344827, 0.023255813953488372, 0.00366300366300365, 0.0]\n",
      "[0.027701778385772924, 0.027419354838709664, 0.03218657029331318, 0.16666666666666669]\n",
      "[0.0, 0.0, 0.002331002331002363, 0.0]\n",
      "local_EO= [0.16666666666666663, 0.3666666666666667, 0.023255813953488372, 0.16666666666666669, 0.002331002331002363]\n",
      "local_EO_avg_post= 0.14511736325689814\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vali_tiled_y=np.concatenate((y_tilde_1,y_tilde_2,y_tilde_3,y_tilde_4,y_tilde_5),axis=0)\n",
    "vali_tiled_y=np.reshape(vali_tiled_y,(-1,))\n",
    "global_EO_post=EO_compuatation(vali_tiled_y,vali_y,vali_s)\n",
    "print('global_EO=',global_EO_post)\n",
    "local_EO_list_post=[]\n",
    "local_EO_1=EO_compuatation(y_tilde_1,vali_y_1,vali_s_1)\n",
    "local_EO_list_post.append(local_EO_1)\n",
    "local_EO_2=EO_compuatation(y_tilde_2,vali_y_2,vali_s_2)\n",
    "local_EO_list_post.append(local_EO_2)\n",
    "local_EO_3=EO_compuatation(y_tilde_3,vali_y_3,vali_s_3)\n",
    "local_EO_list_post.append(local_EO_3)\n",
    "local_EO_4=EO_compuatation(y_tilde_4,vali_y_4,vali_s_4)\n",
    "local_EO_list_post.append(local_EO_4)\n",
    "local_EO_5=EO_compuatation(y_tilde_5,vali_y_5,vali_s_5)\n",
    "local_EO_list_post.append(local_EO_5)\n",
    "local_EO_avg_post=sum(local_EO_list_post)/len(local_EO_list_post)\n",
    "print('local_EO=',local_EO_list_post)\n",
    "local_EO_avg_post=sum(local_EO_list_post)/len(local_EO_list_post)\n",
    "print('local_EO_avg_post=',local_EO_avg_post)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_acc_post= 0.6834666666666667\n"
     ]
    }
   ],
   "source": [
    "acc_count=np.where(vali_tiled_y==vali_y,1,0)\n",
    "acc_post=sum(acc_count)/len(vali_y)\n",
    "print('global_acc_post=',acc_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## result print\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before_post_processing\n",
      "local_EO_avg: 0.2808166480122705\n",
      "global_EO: 0.14225774225774235\n",
      "global_acc: 0.7709333333333334\n",
      "after_post_processing\n",
      "local_EO_avg: 0.14511736325689814\n",
      "global_EO: 0.04560622914349277\n",
      "global_acc: 0.6834666666666667\n"
     ]
    }
   ],
   "source": [
    "print('before_post_processing')\n",
    "print('local_EO_avg:', local_EO_avg)\n",
    "print('global_EO:', vali_EO)\n",
    "print('global_acc:', acc)\n",
    "print('after_post_processing')\n",
    "print('local_EO_avg:', local_EO_avg_post)\n",
    "print('global_EO:', global_EO_post)\n",
    "print('global_acc:', acc_post)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run (g,l):\n",
    "#     x=LP_EO(g,l)\n",
    "#     beta=np.zeros((5,2,5))\n",
    "#     for c in range(5):\n",
    "#             for a in range(2):\n",
    "#                 C=[0,0,0,0,0]\n",
    "#                 A=np.array([[1,1,1,1,1],[TP[c,a,0],1,0,0,0],[TP[c,a,1],0,1,0,0],[TP[c,a,2],0,0,1,0],[TP[c,a,3],0,0,0,1]])\n",
    "#                 b=np.array([1,x[c,a,0],x[c,a,1],x[c,a,2],x[c,a,3]])\n",
    "#                 b=np.reshape(b,(5,1))\n",
    "#                 res = linprog(C, A_eq=A, b_eq=b, bounds=(0,1))\n",
    "#                 beta_ac=res.x\n",
    "#                 beta[c,a,:]=beta_ac\n",
    "#                 e=np.matmul(A,beta_ac)\n",
    "#     beta=np.reshape(beta,(5,2,5))\n",
    "#     y_tilde_1=[]\n",
    "#     y_tilde_2=[]\n",
    "#     y_tilde_3=[]\n",
    "#     y_tilde_4=[]\n",
    "#     y_tilde_5=[]\n",
    "\n",
    "#     # Example usage\n",
    "#     for c, (feature, label, sensitive) in enumerate([(vali_x_1, vali_y_1, vali_s_1), (vali_x_2, vali_y_2, vali_s_2), (vali_x_3, vali_y_3, vali_s_3), (vali_x_4, vali_y_4, vali_s_4), (vali_x_5, vali_y_5, vali_s_5)]):\n",
    "#         y_pred=global_model.predict(feature)\n",
    "#         y_pred=tf.argmax(y_pred, axis=1)\n",
    "#         for i in range (len(y_pred)):\n",
    "#             a=sensitive[i]\n",
    "#             y_hat=y_pred[i]\n",
    "#             y_tilde=compute_tilde_Y(c,a,y_hat,beta)\n",
    "#             if c==0:\n",
    "#                 y_tilde_1.append(y_tilde)\n",
    "#             elif c==1:\n",
    "#                 y_tilde_2.append(y_tilde)\n",
    "#             elif c==2:\n",
    "#                 y_tilde_3.append(y_tilde)\n",
    "#             elif c==3:\n",
    "#                 y_tilde_4.append(y_tilde)\n",
    "#             elif c==4:\n",
    "#                 y_tilde_5.append(y_tilde)\n",
    "#     vali_tiled_y=np.concatenate((y_tilde_1,y_tilde_2,y_tilde_3,y_tilde_4,y_tilde_5),axis=0)\n",
    "#     vali_tiled_y=np.reshape(vali_tiled_y,(-1,))\n",
    "#     global_EO_post=EO_compuatation(vali_tiled_y,vali_y,vali_s)\n",
    "#     local_EO_list_post=[]\n",
    "#     local_EO_1=EO_compuatation(y_tilde_1,vali_y_1,vali_s_1)\n",
    "#     local_EO_list_post.append(local_EO_1)\n",
    "#     local_EO_2=EO_compuatation(y_tilde_2,vali_y_2,vali_s_2)\n",
    "#     local_EO_list_post.append(local_EO_2)\n",
    "#     local_EO_3=EO_compuatation(y_tilde_3,vali_y_3,vali_s_3)\n",
    "#     local_EO_list_post.append(local_EO_3)\n",
    "#     local_EO_4=EO_compuatation(y_tilde_4,vali_y_4,vali_s_4)\n",
    "#     local_EO_list_post.append(local_EO_4)\n",
    "#     local_EO_5=EO_compuatation(y_tilde_5,vali_y_5,vali_s_5)\n",
    "#     local_EO_list_post.append(local_EO_5)\n",
    "#     local_EO_avg_post=sum(local_EO_list_post)/len(local_EO_list_post)\n",
    "#     print('local_EO=',local_EO_list_post)\n",
    "#     local_EO_avg_post=sum(local_EO_list_post)/len(local_EO_list_post)\n",
    "#     print('local_EO_avg_post=',local_EO_avg_post)\n",
    "#     acc_count=np.where(vali_tiled_y==vali_y,1,0)\n",
    "#     acc_post=sum(acc_count)/len(vali_y)\n",
    "#     print('global_EO=',global_EO_post)\n",
    "#     print('global_acc_post=',acc_post)\n",
    "#     return local_EO_avg_post,global_EO_post,acc_post\n",
    "# local_EO_avg_post,global_EO_post,acc_post=run(0.1,0.1)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import scipy.stats as stats\n",
    "# def compute_confidence_interval(data, confidence=0.95):\n",
    "#     \"\"\"\n",
    "#     Compute the confidence interval for a given list of numbers.\n",
    "\n",
    "#     Parameters:\n",
    "#     data (list or array-like): List of numbers.\n",
    "#     confidence (float): Confidence level for the interval.\n",
    "\n",
    "#     Returns:\n",
    "#     tuple: Mean and the margin of error for the confidence interval.\n",
    "#     \"\"\"\n",
    "#     data = np.array(data)\n",
    "#     n = len(data)\n",
    "#     mean = np.mean(data)\n",
    "#     std_err = stats.sem(data)  # Standard error of the mean\n",
    "#     margin_of_error = std_err * stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "\n",
    "#     return mean, margin_of_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_map=[]\n",
    "# lcoal_map=[]\n",
    "# global_map=[]\n",
    "# for l in [0.05,0.1,0.15,0.2,0.25,0.3]:\n",
    "#     for g in [0.05,0.1,0.15,0.2,0.25,0.3]:\n",
    "#         acc_list=[]\n",
    "#         local_list=[]\n",
    "#         global_list=[]\n",
    "#         for _ in range (20):\n",
    "#             local_SP,post_SP,acc=run(g,l)   \n",
    "#             acc_list.append(acc)\n",
    "#             local_list.append(local_SP)\n",
    "#             global_list.append(post_SP)\n",
    "#         acc_average_5_runs=np.mean(acc_list)\n",
    "#         local_average_5_runs=np.mean(local_list)\n",
    "#         global_average_5_runs=np.mean(global_list)\n",
    "#         # mean_1, margin_1 = compute_confidence_interval(acc_list)\n",
    "#         # mean_2, margin_2 = compute_confidence_interval(local_list)\n",
    "#         # mean_3, margin_3 = compute_confidence_interval(global_list)\n",
    "\n",
    "\n",
    "#         acc_map.append(acc_average_5_runs)\n",
    "#         lcoal_map.append(local_average_5_runs)\n",
    "#         global_map.append(global_average_5_runs)\n",
    "# print(acc_map)\n",
    "# print(lcoal_map)\n",
    "# print(global_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_map=np.reshape(acc_map,(6,6))\n",
    "# local_map=np.reshape(lcoal_map,(6,6))\n",
    "# global_map=np.reshape(global_map,(6,6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Define the global disparity (x-axis) and local disparity (y-axis)\n",
    "# global_disparity =[0.05,0.10,0.15,0.20,0.25,0.3]\n",
    "# local_disparity = [0.05,0.10,0.15,0.20,0.25,0.3]\n",
    "\n",
    "# # Create a 5x5 matrix representing the mean accuracy values (example data)\n",
    "# accuracy_mean_matrix = np.array(acc_map\n",
    "# )\n",
    "\n",
    "# # Create a 5x5 matrix representing the variance of accuracy values (example data)\n",
    "\n",
    "# # Create the heatmap plot\n",
    "# fig, ax = plt.subplots()\n",
    "\n",
    "# # Plot the mean accuracy matrix using imshow\n",
    "# cax = ax.imshow(accuracy_mean_matrix, cmap='Blues', interpolation='nearest', aspect='auto', origin='lower', alpha=0.8)\n",
    "\n",
    "# # Set the labels\n",
    "# ax.set_xticks(np.arange(len(global_disparity)))\n",
    "# ax.set_yticks(np.arange(len(local_disparity)))\n",
    "# ax.set_xticklabels(global_disparity)\n",
    "# ax.set_yticklabels(local_disparity)\n",
    "\n",
    "# # Label axes and ensure they start from common 0\n",
    "# # ax.set_xlabel('Global Disparity $\\epsilon$')\n",
    "# # ax.set_ylabel('Local Disparity $\\epsilon_c$')\n",
    "\n",
    "# # Add color bar for accuracy mean\n",
    "# cbar = plt.colorbar(cax, ax=ax)\n",
    "# cbar.set_label('Avg-Acc')\n",
    "\n",
    "# # Set font size for text inside blocks\n",
    "# font_size = 8  # Adjust this value for different sizes\n",
    "\n",
    "# # Annotate each block with the mean  variance\n",
    "# for i in range(len(local_disparity)-2):\n",
    "#     for j in range(len(global_disparity)):\n",
    "#         mean = accuracy_mean_matrix[i+2, j]\n",
    "#         ax.text(j, i+2, f'{mean:.3f}',\n",
    "#                 ha='center', va='center', color='white', fontsize=14)\n",
    "# for i in range(2):\n",
    "#     for j in range(len(global_disparity)):\n",
    "#         mean = accuracy_mean_matrix[i, j]\n",
    "#         ax.text(j, i, f'{mean:.3f}',\n",
    "#                 ha='center', va='center', color='black', fontsize=14)\n",
    "# for i in range(6):\n",
    "#     for j in range(1):\n",
    "#         mean = accuracy_mean_matrix[i, j]\n",
    "#         ax.text(j, i, f'{mean:.3f}',\n",
    "#                 ha='center', va='center', color='black', fontsize=14)\n",
    "# for i in [2]:\n",
    "#     for j in [1]:\n",
    "#         mean = accuracy_mean_matrix[i, j]\n",
    "#         ax.text(j, i, f'{mean:.3f}',\n",
    "#                 ha='center', va='center', color='black', fontsize=14)\n",
    "# # for i in range(len(local_disparity)):\n",
    "# #     for j in range(2):\n",
    "# #         mean = accuracy_mean_matrix[i, j]\n",
    "# #         ax.text(j, i, f'{mean:.3f}',\n",
    "# #                 ha='center', va='center', color='black', fontsize=12)\n",
    "\n",
    "# # Set the axes to start from a common origin\n",
    "# ax.set_xlim(-0.5, len(global_disparity) - 0.5)\n",
    "# ax.set_ylim(-0.5, len(local_disparity) - 0.5)\n",
    "\n",
    "# # Set title\n",
    "# # ax.set_title('Average Accuracy with different $(\\epsilon_0, \\epsilon_c)$')\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "post_FFL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

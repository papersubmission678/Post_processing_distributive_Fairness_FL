{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn import datasets\n",
    "from tornado.options import define\n",
    "\n",
    "from data_processing import data_processing\n",
    "from scipy.optimize import linprog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fairness pre-specified Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "g=0.01\n",
    "l=0.01\n",
    "m=0.01                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_1, vali_features_1, test_features_1, train_labels_1, vali_labels_1, test_labels_1, train_sensitive_1, vali_sensitive_1, test_sensitive_1,train_features_2,vali_features_2, test_features_2, train_labels_2, vali_labels_2, test_labels_2, train_sensitive_2, vali_sensitive_2, test_sensitive_2=data_processing()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model by FedAvg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape):\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.InputLayer(input_shape=input_shape),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = train_features_1.shape[1:]  \n",
    "global_model = create_model(input_shape)\n",
    "global_model.load_weights('Fed_Avg.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluations of Model Trained By FedAvg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_demographic_parity(y_true, y_pred, groups):\n",
    "    y_true = np.asarray(y_true).flatten()\n",
    "    y_pred = np.asarray(y_pred).flatten()\n",
    "    groups = np.asarray(groups).flatten()\n",
    "    white_pr = np.mean(y_pred[groups == 1])\n",
    "    black_pr = np.mean(y_pred[groups == 0])\n",
    "    DP=abs(white_pr-black_pr)\n",
    "    return DP\n",
    "def compute_accuracy(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true).flatten()\n",
    "    y_pred = np.asarray(y_pred).flatten()\n",
    "    accuracy = np.mean(y_true == y_pred)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_SP=[]\n",
    "for client_data in [(test_features_1, test_labels_1, test_sensitive_1), (test_features_2, test_labels_2, test_sensitive_2)]:\n",
    "    vali_features, vali_labels,vali_sensitive = client_data\n",
    "    y_val_pred=global_model.predict(vali_features)\n",
    "    y_val_pred=np.where(y_val_pred<0.5,0,1)\n",
    "    DP=compute_demographic_parity(vali_labels,y_val_pred,vali_sensitive)\n",
    "    local_SP.append(DP)\n",
    "local_SP_avg_1=np.mean(local_SP)\n",
    "global_features=np.concatenate((test_features_1,test_features_2),axis=0)\n",
    "global_labels=np.concatenate((test_labels_1,test_labels_2),axis=0)\n",
    "global_sensitive=np.concatenate((test_sensitive_1,test_sensitive_2),axis=0)\n",
    "y_global_pred=global_model.predict(global_features)\n",
    "y_global_pred=np.where(y_global_pred<0.5,0,1)\n",
    "global_SP_1=compute_demographic_parity(global_labels,y_global_pred,global_sensitive)\n",
    "y_global_pred=y_global_pred.flatten()\n",
    "global_labels=global_labels.flatten()\n",
    "acc_fed=compute_accuracy(global_labels,y_global_pred)\n",
    "\n",
    "y_1_pred=global_model.predict(test_features_1)  \n",
    "y_1_pred=np.where(y_1_pred<0.5,0,1)\n",
    "y_1_pred=y_1_pred.flatten()\n",
    "acc_1=compute_accuracy(test_labels_1,y_1_pred)\n",
    "y_2_pred=global_model.predict(test_features_2)\n",
    "y_2_pred=np.where(y_2_pred<0.5,0,1)\n",
    "y_2_pred=y_2_pred.flatten()\n",
    "acc_2=compute_accuracy(test_labels_2,y_2_pred)\n",
    "client_disparity_1=abs(acc_1-acc_2)\n",
    "\n",
    "print('###################################')\n",
    "print('local disparity',local_SP_avg_1)\n",
    "print('global disparity',global_SP_1)\n",
    "print('client disparity',client_disparity_1)\n",
    "print('global accuracy',acc_fed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the statistics for fair LP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.29027511 0.03525272]\n",
      "  [0.46628279 0.19603327]]\n",
      "\n",
      " [[0.00108765 0.00089571]\n",
      "  [0.00204734 0.0081254 ]]]\n",
      "1.0\n",
      "[[0.32552783 0.66231606]\n",
      " [0.00198337 0.01017274]]\n",
      "[[[0.29384715 0.03568653]\n",
      "  [0.47202073 0.1984456 ]]\n",
      "\n",
      " [[0.08947368 0.07368421]\n",
      "  [0.16842105 0.66842105]]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## p(y,a|c)\n",
    "def p_computation(label, sensitive_attribute, y, a):\n",
    "    label=np.reshape(label,(-1,))\n",
    "    N=len(label)\n",
    "    sensitive_attribute=np.reshape(sensitive_attribute,(-1,))# Total number of samples\n",
    "    # Create a boolean mask for where both conditions are met\n",
    "    mask = (label == y) & (sensitive_attribute == a)\n",
    "    # print(mask)\n",
    "    # print(\"Number of matching samples:\", np.sum(mask))\n",
    "    \n",
    "    # Compute the probability\n",
    "    p_y_ac = np.sum(mask) / N\n",
    "    return p_y_ac\n",
    "#community_1\n",
    "## p(y,a,c)\n",
    "S= np.zeros((2,2,2))\n",
    "p=[len(vali_features_1)/(len(vali_features_1)+len(vali_features_2)),len(vali_features_2)/(len(vali_features_1)+len(vali_features_2))]\n",
    "for c, (feature, label, sensitive) in enumerate([(vali_features_1, vali_labels_1, vali_sensitive_1), (vali_features_2, vali_labels_2,vali_sensitive_2)]):\n",
    "    for a in range(2):\n",
    "        for y in range(2):\n",
    "            S[c,a,y]=p[c]*p_computation(label,sensitive,y,a)\n",
    "alpha_ac=np.sum(S,axis=-1)\n",
    "\n",
    "\n",
    "L=np.zeros((2,2,2))\n",
    "for c, (feature, label, sensitive) in enumerate([(vali_features_1, vali_labels_1, vali_sensitive_1), (vali_features_2, vali_labels_2,vali_sensitive_2)]):\n",
    "    for a in range(2):\n",
    "        for y in range(2):\n",
    "            L[c,a,y]=p_computation(label,sensitive,y,a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TP of the optimal predictor\n",
    "\n",
    "TP= np.zeros((2,2,2))\n",
    "def compute_TP (y_pred,label, sensitive_attribute, y, a):\n",
    "    y_pred=np.reshape(y_pred,(-1,))\n",
    "    label=np.reshape(label,(-1,))\n",
    "    N=len(y_pred)\n",
    "    sensitive_attribute=np.reshape(sensitive_attribute,(-1,))# Total number of samples\n",
    "    # Create a boolean mask for where both conditions are met\n",
    "    count_1= (y_pred == y) & (label == y) & (sensitive_attribute == a)\n",
    "    count_2 = (label== y) & (sensitive_attribute == a)\n",
    "    print(np.sum(count_1),np.sum(count_2))\n",
    "    \n",
    "    # Compute the probability\n",
    "    TP_y_ac = np.sum(count_1) / np.sum(count_2)\n",
    "    return TP_y_ac\n",
    "for c, (feature, label, sensitive) in enumerate([(vali_features_1, vali_labels_1, vali_sensitive_1), (vali_features_2, vali_labels_2,vali_sensitive_2)]):\n",
    "    y_pred=global_model.predict(feature)\n",
    "    y_pred=np.where(y_pred<0.5,0,1)\n",
    "    for a in range(2):\n",
    "        for y in range(2):\n",
    "            TP[c,a,y]=compute_TP(y_pred,label,sensitive,y,a)\n",
    "print(TP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_alpha(label, sensitive_attribute, y, a):\n",
    "    label=np.reshape(label,(-1,))\n",
    "    N=len(label)\n",
    "    sensitive_attribute=np.reshape(sensitive_attribute,(-1,))# Total number of samples\n",
    "    # Create a boolean mask for where both conditions are met\n",
    "    mask = (label == y) & (sensitive_attribute == a)\n",
    "    # print(mask)\n",
    "    # print(\"Number of matching samples:\", np.sum(mask))\n",
    "    \n",
    "    # Compute the probability\n",
    "    alpha_y_ac = np.sum(mask) / N\n",
    "    return alpha_y_ac\n",
    "alpha_a_y = np.zeros((2,2))\n",
    "for a in range(2):\n",
    "    for y in range(2):\n",
    "        alpha_a_y[a,y]=compute_alpha(global_labels,global_sensitive,y,a)\n",
    "print(alpha_a_y)\n",
    "def compute_e(sensitive,a):\n",
    "    sensitive=np.reshape(sensitive,(-1,))\n",
    "    N=len(sensitive)\n",
    "    mask = (sensitive == a)\n",
    "    e_a = np.sum(mask) / N\n",
    "    return e_a\n",
    "e_a=np.zeros(2)\n",
    "for a in range(2):\n",
    "    e_a[a]=compute_e(global_sensitive,a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LP for statistical Parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LP_DP(e_g,e_l,e_c):\n",
    "# define the objective function\n",
    "    C=[]\n",
    "    for c in range(2):\n",
    "        for a in range(2):\n",
    "            for y in range(2):\n",
    "                C.append(-S[c,a,y])\n",
    "\n",
    "    #define the global fairness constraints\n",
    "\n",
    "\n",
    "\n",
    "    ## global fairness constraints\n",
    "    A_1=[-S[0,0,0]/e_a[0],S[0,0,1]/e_a[0],S[0,1,0]/e_a[1],-S[0,1,1]/e_a[1], -S[1,0,0]/e_a[0],S[1,0,1]/e_a[0], S[1,1,0]/e_a[1],-S[1,1,1]/e_a[1]]\n",
    "\n",
    "    b_1=[-S[0,0,0]/e_a[0]+S[0,1,0]/e_a[1]-S[1,0,0]/e_a[0]+S[1,1,0]/e_a[1]]\n",
    "\n",
    "\n",
    "    ## local fairness constraints\n",
    "    A_2=[-S[0,0,0]/alpha_ac[0,0],S[0,0,1]/alpha_ac[0,0],S[0,1,0]/alpha_ac[0,1],-S[0,1,1]/alpha_ac[0,1],0,0,0,0]\n",
    "\n",
    "    b_2=[-S[0,0,0]/alpha_ac[0,0]+S[0,1,0]/alpha_ac[0,1]]\n",
    "\n",
    "\n",
    "    A_3=[0,0,0,0,-S[1,0,0]/alpha_ac[1,0],S[1,0,1]/alpha_ac[1,0],S[1,1,0]/alpha_ac[1,1],-S[1,1,1]/alpha_ac[1,1]]\n",
    "\n",
    "    b_3=[-S[1,0,0]/alpha_ac[1,0]+S[1,1,0]/alpha_ac[1,1]]\n",
    "    ## client fairness constraints\n",
    "    A_4=[L[0,0,0],L[0,0,1],L[0,1,0],L[0,1,1],-L[1,0,0],-L[1,0,1],-L[1,1,0],-L[1,1,1]]\n",
    "    b_4=[0]\n",
    "    A_1=np.array(A_1)\n",
    "    A_2=np.array(A_2) \n",
    "    A_3=np.array(A_3)\n",
    "    A_4=np.array(A_4)\n",
    "    \n",
    "    print(A_4)\n",
    "\n",
    "\n",
    "    ## constraints for the feasible region of LP\n",
    "    def K_ac_compute(a,c):\n",
    "        K_ac=np.zeros((3,2))\n",
    "        l_ac=np.zeros((3,1))\n",
    "        K_ac[0:]=[-1,-1]\n",
    "        K_ac[1:]=[(1-TP[c,a,1]), TP[c,a,0]]\n",
    "        K_ac[2:]=[TP[c,a,1], (1-TP[c,a,0])]\n",
    "        l_ac[0:]=[-1]\n",
    "        l_ac[1:]=[TP[c,a,0]]\n",
    "        l_ac[2:]=[TP[c,a,1]]\n",
    "        return K_ac,l_ac\n",
    "    \n",
    "    # Define the submatrices k_01, k_11, k_02, k_12 as 3x2 matrices\n",
    "    k_01 ,l_01= K_ac_compute(0,0)\n",
    "    k_11 ,l_11= K_ac_compute(1,0)\n",
    "    k_02 ,l_02= K_ac_compute(0,1)\n",
    "    k_12 ,l_12= K_ac_compute(1,1)\n",
    "    \n",
    "    \n",
    "    # Initialize the large matrix with zeros\n",
    "    M = np.zeros((12, 8))  # 4 blocks of 3x2 matrices; resulting in a 12x8 matrix\n",
    "    l=np.zeros((12,1))\n",
    "    # Place each submatrix on the diagonal\n",
    "    M[0:3, 0:2] = k_01  # Place k_01 in the top-left\n",
    "    M[3:6, 2:4] = k_11  # Place k_11 in the next diagonal block\n",
    "    M[6:9, 4:6] = k_02  # Place k_02 in the next diagonal block\n",
    "    M[9:12, 6:8] = k_12 # Place k_12 in the bottom-right block\n",
    "    \n",
    "    l[0:3]=l_01\n",
    "    l[3:6]=l_11\n",
    "    l[6:9]=l_02\n",
    "    l[9:12]=l_12\n",
    "    # print(np.shape(A_1),np.shape(A_2),np.shape(A_3),np.shape(M))\n",
    "    # combine the constraints\n",
    "    A=np.vstack((A_1,A_2,A_3,A_4, -A_1, -A_2, -A_3,-A_4,M))\n",
    "    trail_1=[e_g]\n",
    "    trail_2=[e_l]\n",
    "    trail_3=[e_c]\n",
    "    b_1=np.reshape(b_1,(1,1))\n",
    "    b_2=np.reshape(b_2,(1,1))\n",
    "    b_3=np.reshape(b_3,(1,1))\n",
    "    b_4=np.reshape(b_4,(1,1))\n",
    "    # print(A)\n",
    "    # print(b_local)\n",
    "    b_r=np.vstack((trail_1,trail_2,trail_2,trail_3, trail_1,trail_2,trail_2,trail_3, np.zeros((12,1))))\n",
    "    b_t=np.vstack((b_1,b_2,b_3,b_4,-b_1,-b_2,-b_3,-b_4,l))\n",
    "    b=b_r+b_t\n",
    "    # print(b)\n",
    "    res = linprog(C, A_ub=A, b_ub=b)\n",
    "    x=res.x\n",
    "    l=res.fun\n",
    "    x=np.reshape(x,(len(x),1))\n",
    "    x=np.reshape(x,(2,2,2))\n",
    "    return x\n",
    "x=LP_DP(g,l,m)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LP for Equalized Odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def LP_EO(e_0,e_c):\n",
    "# # define the objective function\n",
    "#     C=[]\n",
    "#     for c in range(2):\n",
    "#         for a in range(2):\n",
    "#             for y in range(2):\n",
    "#                 C.append(-S[c,a,y])\n",
    "#     #define the global fairness constraints\n",
    "#     N=2\n",
    "#     K=2\n",
    "#     A_1=[]\n",
    "#     A_2=[]\n",
    "#     for c in range(2):\n",
    "#         for a in range(2):\n",
    "#             for y in range(2):\n",
    "#                 if  a==0 and y==0:\n",
    "#                     A_1.append(-S[c,a,y]/alpha_a_y[a,y])\n",
    "#                     # print(S[c,a,y],alpha_a_y[a,y])\n",
    "#                 elif  a==1 and y==0:\n",
    "#                     A_1.append(S[c,a,y]/alpha_a_y[a,y])\n",
    "    \n",
    "#                 else:\n",
    "#                     A_1.append(0)\n",
    "#                     print(2)\n",
    "    \n",
    "#     for c in range(2):\n",
    "#         for a in range(2):\n",
    "#             for y in range(2):\n",
    "#                 if c==c and a==0 and y==1:\n",
    "#                     A_2.append(-S[c,a,y]/alpha_a_y[a,y])\n",
    "#                 elif c==c and a==1 and y==1:\n",
    "#                     A_2.append(S[c,a,y]/alpha_a_y[a,y])\n",
    "#                 else:\n",
    "#                     A_2.append(0)\n",
    "#     # define local fairness constraints\n",
    "#     basis_vectors = np.eye(N)  # Identity matrix of size 50x50, where each row is a basis vector\n",
    "#     A_1=np.array(A_1)\n",
    "#     A_2=np.array(A_2)\n",
    "#     # print(np.shape(A_1),np.shape(A_2))\n",
    "#     # To access e_i, you can use basis_vectors[i], e.g., e_0 is basis_vectors[0]\n",
    "    \n",
    "#     # Define zero vector in R^50\n",
    "#     zero_vec = np.zeros((N,N))\n",
    "    \n",
    "    \n",
    "#     # Construct the matrix\n",
    "#     # First row: [e_0, e_1, 0, 0]\n",
    "#     row1 = np.hstack((basis_vectors, -basis_vectors, zero_vec, zero_vec))\n",
    "    \n",
    "#     # Second row: [0, 0, e_0, e_1]\n",
    "#     row2 = np.hstack((zero_vec, zero_vec, basis_vectors, -basis_vectors))\n",
    "    \n",
    "#     # Combine rows into a single matrix\n",
    "#     A_3 = np.vstack((row1, row2))\n",
    "#     # define the property of dervied outcome predictor\n",
    "#     def K_ac_compute(a,c):\n",
    "#         K_ac=np.zeros((3,2))\n",
    "#         l_ac=np.zeros((3,1))\n",
    "#         K_ac[0:]=[-1,-1]\n",
    "#         K_ac[1:]=[(1-TP[c,a,1]), TP[c,a,0]]\n",
    "#         K_ac[2:]=[TP[c,a,1], (1-TP[c,a,0])]\n",
    "#         l_ac[0:]=[-1]\n",
    "#         l_ac[1:]=[TP[c,a,0]]\n",
    "#         l_ac[2:]=[TP[c,a,1]]\n",
    "#         return K_ac,l_ac\n",
    "    \n",
    "#     # Define the submatrices k_01, k_11, k_02, k_12 as 3x2 matrices\n",
    "#     k_01 ,l_01= K_ac_compute(0,0)\n",
    "#     k_11 ,l_11= K_ac_compute(1,0)\n",
    "#     k_02 ,l_02= K_ac_compute(0,1)\n",
    "#     k_12 ,l_12= K_ac_compute(1,1)\n",
    "    \n",
    "    \n",
    "#     # Initialize the large matrix with zeros\n",
    "#     M = np.zeros((12, 8))  # 4 blocks of 3x2 matrices; resulting in a 12x8 matrix\n",
    "#     l=np.zeros((12,1))\n",
    "#     # Place each submatrix on the diagonal\n",
    "#     M[0:3, 0:2] = k_01  # Place k_01 in the top-left\n",
    "#     M[3:6, 2:4] = k_11  # Place k_11 in the next diagonal block\n",
    "#     M[6:9, 4:6] = k_02  # Place k_02 in the next diagonal block\n",
    "#     M[9:12, 6:8] = k_12 # Place k_12 in the bottom-right block\n",
    "    \n",
    "#     l[0:3]=l_01\n",
    "#     l[3:6]=l_11\n",
    "#     l[6:9]=l_02\n",
    "#     l[9:12]=l_12\n",
    "#     # print(np.shape(A_1),np.shape(A_2),np.shape(A_3),np.shape(M))\n",
    "#     # combine the constraints\n",
    "#     A=np.vstack((A_1,A_2,A_3, -A_1, -A_2, -A_3,M))\n",
    "#     # print(A)\n",
    "#     b_global=e_0*np.ones((2,1))\n",
    "#     b_local=e_c*np.ones((4,1))\n",
    "#     # print(b_local)\n",
    "#     b=np.vstack((b_global,b_local,b_global,b_local,l))\n",
    "#     # print(b)\n",
    "#     res = linprog(C, A_ub=A, b_ub=b)\n",
    "#     x=res.x\n",
    "#     x=np.reshape(x,(2,2,2))\n",
    "#     print(res.fun)\n",
    "#     return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LP for equal opportunity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def LP_EOp(e_0,e_c):\n",
    "# # define the objective function\n",
    "#     C=[]\n",
    "#     for c in range(2):\n",
    "#         for a in range(2):\n",
    "#             for y in range(2):\n",
    "#                 C.append(-S[c,a,y])\n",
    "#     #define the global fairness constraints\n",
    "#     N=2\n",
    "#     K=2\n",
    "#     A_2=[]\n",
    "\n",
    "    \n",
    "#     for c in range(2):\n",
    "#         for a in range(2):\n",
    "#             for y in range(2):\n",
    "#                 if c==c and a==0 and y==1:\n",
    "#                     A_2.append(-S[c,a,y]/alpha_a_y[a,y])\n",
    "#                 elif c==c and a==1 and y==1:\n",
    "#                     A_2.append(S[c,a,y]/alpha_a_y[a,y])\n",
    "#                 else:\n",
    "#                     A_2.append(0)\n",
    "#     # define local fairness constraints\n",
    "#     basis_vectors = [[0,1]]\n",
    "#     basis_vectors=np.reshape(basis_vectors,(1,2))# Identity matrix of size 50x50, where each row is a basis vector\n",
    "#     A_2=np.array(A_2)\n",
    "#     # print(np.shape(A_1),np.shape(A_2))\n",
    "#     # To access e_i, you can use basis_vectors[i], e.g., e_0 is basis_vectors[0]\n",
    "    \n",
    "#     # Define zero vector in R^50\n",
    "#     zero_vec = np.zeros((1,2))\n",
    "    \n",
    "    \n",
    "#     # Construct the matrix\n",
    "#     # First row: [e_0, e_1, 0, 0]\n",
    "#     row1 = np.hstack((basis_vectors, -basis_vectors, zero_vec, zero_vec))\n",
    "    \n",
    "#     # Second row: [0, 0, e_0, e_1]\n",
    "#     row2 = np.hstack((zero_vec, zero_vec, basis_vectors, -basis_vectors))\n",
    "    \n",
    "#     # Combine rows into a single matrix\n",
    "#     A_3 = np.vstack((row1, row2))\n",
    "#     # define the property of dervied outcome predictor\n",
    "#     def K_ac_compute(a,c):\n",
    "#         K_ac=np.zeros((3,2))\n",
    "#         l_ac=np.zeros((3,1))\n",
    "#         K_ac[0:]=[-1,-1]\n",
    "#         K_ac[1:]=[(1-TP[c,a,1]), TP[c,a,0]]\n",
    "#         K_ac[2:]=[TP[c,a,1], (1-TP[c,a,0])]\n",
    "#         l_ac[0:]=[-1]\n",
    "#         l_ac[1:]=[TP[c,a,0]]\n",
    "#         l_ac[2:]=[TP[c,a,1]]\n",
    "#         return K_ac,l_ac\n",
    "    \n",
    "#     # Define the submatrices k_01, k_11, k_02, k_12 as 3x2 matrices\n",
    "#     k_01 ,l_01= K_ac_compute(0,0)\n",
    "#     k_11 ,l_11= K_ac_compute(1,0)\n",
    "#     k_02 ,l_02= K_ac_compute(0,1)\n",
    "#     k_12 ,l_12= K_ac_compute(1,1)\n",
    "    \n",
    "    \n",
    "#     # Initialize the large matrix with zeros\n",
    "#     M = np.zeros((12, 8))  # 4 blocks of 3x2 matrices; resulting in a 12x8 matrix\n",
    "#     l=np.zeros((12,1))\n",
    "#     # Place each submatrix on the diagonal\n",
    "#     M[0:3, 0:2] = k_01  # Place k_01 in the top-left\n",
    "#     M[3:6, 2:4] = k_11  # Place k_11 in the next diagonal block\n",
    "#     M[6:9, 4:6] = k_02  # Place k_02 in the next diagonal block\n",
    "#     M[9:12, 6:8] = k_12 # Place k_12 in the bottom-right block\n",
    "    \n",
    "#     l[0:3]=l_01\n",
    "#     l[3:6]=l_11\n",
    "#     l[6:9]=l_02\n",
    "#     l[9:12]=l_12\n",
    "#     # print(np.shape(A_1),np.shape(A_2),np.shape(A_3),np.shape(M))\n",
    "#     # combine the constraints\n",
    "#     A=np.vstack((A_2,A_3, -A_2, -A_3,M))\n",
    "#     # print(A)\n",
    "#     b_global=e_0*np.ones((1,1))\n",
    "#     b_local=e_c*np.ones((2,1))\n",
    "#     # print(b_local)\n",
    "#     b=np.vstack((b_global,b_local,b_global,b_local,l))\n",
    "#     # print(b)\n",
    "#     res = linprog(C, A_ub=A, b_ub=b)\n",
    "#     x=res.x\n",
    "#     x=np.reshape(x,(2,2,2))\n",
    "#     print(res.fun)\n",
    "#     return x\n",
    "# x=LP_EOp(0.0001,0.0001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Fair Outcome Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta=np.zeros((2,2,3))\n",
    "for a in range(2):\n",
    "        for c in range(2):\n",
    "            A=np.array([[1,1,1],[TP[c,a,0],1,0],[TP[c,a,1],0,1]])\n",
    "            b=np.array([1,x[c,a,0],x[c,a,1]])\n",
    "            beta_ac=np.linalg.solve(A,b)\n",
    "            beta[c,a,:]=beta_ac\n",
    "beta=np.reshape(beta,(2,2,3))\n",
    "# print(beta)\n",
    "\n",
    "def compute_tilde_Y(c,a, Y_hat, y_values, beta):\n",
    "    \"\"\"\n",
    "    Compute \\widetilde{Y}_{\\boldsymbol{\\beta}_{ac}}(x,a,c) based on given probabilities.\n",
    "\n",
    "    Parameters:\n",
    "    - beta_ac: Dictionary with keys 'beta_0' and 'beta_y' for probabilities.\n",
    "    - Y_hat: The predicted value \\hat{Y}(x,a,c).\n",
    "    - y_values: List or array of possible y values in \\mathcal{Y}.\n",
    "\n",
    "    Returns:\n",
    "    - tilde_Y: The computed value of \\widetilde{Y}.\n",
    "    \"\"\"\n",
    "\n",
    "    beta_0 = beta[c, a, 0]  # Probability for Y_hat\n",
    "    beta_y = beta[c, a, 1:] \n",
    "    # Probabilities for other y in \\mathcal{Y}\n",
    "    \n",
    "    # Normalize probabilities\n",
    "    \n",
    "    # Generate a random number\n",
    "    rand_val = np.random.rand()\n",
    "    \n",
    "    # Determine the output based on random value\n",
    "    if rand_val < beta_0:\n",
    "        return Y_hat\n",
    "    else:\n",
    "        cumulative_prob = beta_0\n",
    "        for y in y_values:\n",
    "            cumulative_prob += beta_y[y]\n",
    "            if rand_val < cumulative_prob:\n",
    "                return y\n",
    "y_tilde_1=[]\n",
    "y_tilde_2=[]\n",
    "# Example usage\n",
    "for c, (feature, label, sensitive) in enumerate([(vali_features_1, vali_labels_1, vali_sensitive_1), (vali_features_2, vali_labels_2,vali_sensitive_2)]):\n",
    "    y_pred=global_model.predict(feature)\n",
    "    y_pred=np.where(y_pred<0.5,0,1)\n",
    "    for i in range (len(y_pred)):\n",
    "        a=int(sensitive[i])\n",
    "        y_hat=y_pred[i]\n",
    "        y_tilde=compute_tilde_Y(c,a,y_hat,[0,1],beta)\n",
    "        y_tilde=int(y_tilde)\n",
    "        if c==0:\n",
    "            y_tilde_1.append(y_tilde)\n",
    "        elif c==1:\n",
    "            y_tilde_2.append(y_tilde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tilde_1=[]\n",
    "y_tilde_2=[]\n",
    "# Example usage\n",
    "for c, (feature, label, sensitive) in enumerate([(test_features_1, test_labels_1, test_sensitive_1), (test_features_2, test_labels_2,test_sensitive_2)]):\n",
    "    y_pred=global_model.predict(feature)\n",
    "    y_pred=np.where(y_pred<0.5,0,1)\n",
    "    for i in range (len(y_pred)):\n",
    "        a=int(sensitive[i])\n",
    "        y_hat=y_pred[i]\n",
    "        y_tilde=compute_tilde_Y(c,a,y_hat,[0,1],beta)\n",
    "        y_tilde=int(y_tilde)\n",
    "        if c==0:\n",
    "            y_tilde_1.append(y_tilde)\n",
    "        elif c==1:\n",
    "            y_tilde_2.append(y_tilde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local SP [0.33144700843688624, 0.375]\n",
      "local SP 0.3532235042184431\n",
      "global SP 0.33587339572525327\n",
      "global acc 0.7936329204626881\n",
      "client acc 0.7934715025906736 0.8067226890756303\n",
      "client disparity 0.013251186484956712\n"
     ]
    }
   ],
   "source": [
    "local_SP=[]\n",
    "post_SP_1=compute_demographic_parity(test_labels_1,y_tilde_1,test_sensitive_1)\n",
    "local_SP.append(post_SP_1)\n",
    "post_SP_2=compute_demographic_parity(test_labels_2,y_tilde_2,test_sensitive_2)\n",
    "local_SP.append(post_SP_2)\n",
    "print('local SP',local_SP)\n",
    "local_SP_avg=np.mean(local_SP)\n",
    "print('local SP',local_SP_avg)\n",
    "y_tilde=np.concatenate((y_tilde_1,y_tilde_2),axis=0)\n",
    "global_test_label=np.concatenate((test_labels_1,test_labels_2),axis=0)\n",
    "global_test_sensitive=np.concatenate((test_sensitive_1,test_sensitive_2),axis=0)\n",
    "post_SP=compute_demographic_parity(global_test_label,y_tilde,global_test_sensitive)\n",
    "print('global SP', post_SP)\n",
    "acc_count=np.where(y_tilde==global_test_label,1,0)\n",
    "acc=np.sum(acc_count)/len(acc_count)\n",
    "print('global acc',acc)\n",
    "y_tilde_1=np.array(y_tilde_1)\n",
    "y_tilde_2=np.array(y_tilde_2)\n",
    "acc_1=compute_accuracy(test_labels_1,y_tilde_1)\n",
    "acc_2=compute_accuracy(test_labels_2,y_tilde_2)\n",
    "client_dis=abs(acc_1-acc_2)\n",
    "print('client acc',acc_1,acc_2)\n",
    "print('client disparity',client_dis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## before post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('local SP',local_SP_avg_1)\n",
    "print('global SP',global_SP_1)\n",
    "print('global acc',acc_fed)\n",
    "print('client disparity',client_dis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## after post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('local SP',local_SP_avg) \n",
    "print('global SP',post_SP)\n",
    "print('global acc',acc)\n",
    "print('client disparity', client_dis)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
